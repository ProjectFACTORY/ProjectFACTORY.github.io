Automatically generated by Mendeley Desktop 1.16.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Gupta2012,
abstract = {We propose a nonparametric Bayesian, linear Poisson gamma model for count data and use it for dictionary learning. A key property of this model is that it captures the parts-based representation similar to nonnegative matrix factorization. We present an auxiliary variable Gibbs sampler, which turns the intractable inference into a tractable one. Combining this inference procedure with the slice sampler of Indian buffet process, we show that our model can learn the number of factors automatically. Using synthetic and real-world datasets, we show that the proposed model outperforms other state-of-the-art nonparametric factor models. {\textcopyright} 2012 ICPR Org Committee.},
author = {Gupta, S K and Phung, D and Venkatesh, S},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Proceedings - International Conference on Pattern Recognition - 2012 - Gupta, Phung, Venkatesh - A nonparametric Bayesian Poisson gamma.pdf:pdf},
isbn = {9784990644116},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
keywords = {Feature Reduction and Manifold Learning,Machine Learning and Data Mining,Statistical, Syntactic and Structural Pattern Reco},
number = {Icpr},
pages = {1815--1818},
title = {{A nonparametric Bayesian Poisson gamma model for count data}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874576230{\&}partnerID=40{\&}md5=5b7b4a282515145b382bc8c9e57055dc},
year = {2012}
}
@article{Fevotte2011,
abstract = {This letter describes algorithms for nonnegative matrix factorization (NMF) with the $\beta$-divergence ($\beta$-NMF). The $\beta$-divergence is a family of cost functions parameterized by a single shape parameter $\beta$ that takes the Euclidean distance, the Kullback-Leibler divergence, and the Itakura-Saito divergence as special cases ($\beta$ = 2, 1, 0 respectively). The proposed algorithms are based on a surrogate auxiliary function (a local majorization of the criterion function). We first describe a majorization-minimization algorithm that leads to multiplicative updates, which differ from standard heuristic multiplicative updates by a $\beta$-dependent power exponent. The monotonicity of the heuristic algorithm can, however, be proven for $\beta$ ∈ (0, 1) using the proposed auxiliary function. Then we introduce the concept of the majorization-equalization (ME) algorithm, which produces updates that move along constant level sets of the auxiliary function and lead to larger steps than MM. Simulations on synthetic and real data illustrate t...},
archivePrefix = {arXiv},
arxivId = {arXiv:1010.1763v3},
author = {F{\'{e}}votte, C{\'{e}}dric and Idier, J{\'{e}}r{\^{o}}me},
doi = {10.1162/NECO_a_00168},
eprint = {arXiv:1010.1763v3},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Neural Computation - 2011 - F{\'{e}}votte, Idier - Algorithms for Nonnegative Matrix Factorization with the $\beta$-Divergence.pdf:pdf},
isbn = {9781424418206},
issn = {0899-7667},
journal = {Neural Computation},
keywords = {majorization-equalization,majorization-minimization,me,mm,multiplicative algorithms,nmf,nonnegative matrix factorization,$\beta$ -divergence},
number = {9},
pages = {2421--2456},
pmid = {21671793},
title = {{Algorithms for Nonnegative Matrix Factorization with the $\beta$-Divergence}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/NECO{\_}a{\_}00168},
volume = {23},
year = {2011}
}
@article{Ruiz2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1603.01140v1},
author = {Ruiz, F J R and Titsias, M K and Blei, D M},
eprint = {arXiv:1603.01140v1},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Uncertainty in Artificial Intelligence - 2016 - Ruiz, Titsias, Blei - Overdispersed Black-Box Variational Inference.pdf:pdf},
journal = {Uncertainty in Artificial Intelligence},
title = {{Overdispersed Black-Box Variational Inference}},
year = {2016}
}
@article{Picchini,
archivePrefix = {arXiv},
arxivId = {1512.04831},
author = {Picchini, Umberto and Samson, Adeline},
eprint = {1512.04831},
file = {:home/alumbreras/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Picchini, Samson - Unknown - Coupling stochastic EM and Approximate Bayesian Computation for parameter inference in state-space models.pdf:pdf},
keywords = {hidden markov model,maximum likelihood,monte carlo,saem,sequential,stochastic differential equation},
number = {0},
pages = {1--22},
title = {{Coupling stochastic EM and Approximate Bayesian Computation for parameter inference in state-space models}},
volume = {46}
}
@article{Dayan1995,
abstract = {Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways.},
author = {Dayan, P and Hinton, G.E. E and Neal, R.M. M and Zemel, R.S. S},
doi = {10.1162/neco.1995.7.5.889},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Neural computation - 1995 - Dayan et al. - The helmholtz machine.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Automated,Feedback,Humans,Models,Pattern Recognition,Perception,Perception: physiology,Psychological,Stochastic Processes,Visual},
number = {5},
pages = {889--904},
pmid = {7584891},
title = {{The helmholtz machine}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/7584891$\backslash$nhttp://www.mitpressjournals.org/doi/pdf/10.1162/neco.1995.7.5.889},
volume = {7},
year = {1995}
}
@article{Fevotte2009,
abstract = {We develop an interpretation of nonnegative matrix fac- torization (NMF) methods based on Euclidean distance, Kullback-Leibler and Itakura-Saito divergences in a proba- bilistic framework. We describe how these factorizations are implicit in a well-defined statistical model of superimposed components, either Gaussian or Poisson distributed, and are equivalent to maximum likelihood estimation of either mean, variance or intensity parameters. By treating the components as hidden-variables, NMFalgorithms can be derived in a typ- ical data augmentation setting. This setting can in partic- ular accommodate regularization constraints on the matrix factors through Bayesian priors. We describe multiplicative, Expectation-Maximization, Markov chain Monte Carlo and Variational Bayes algorithms for the NMF problem. This pa- per describes in a unified framework both new and known algorithms and aims at providing statistical insights to NMF.},
author = {F{\'{e}}votte, C{\'{e}}dric and Cemgil, A. Taylan},
file = {:home/alumbreras/Documentos/Mendeley/Papers/European Signal Processing Conference - 2009 - F{\'{e}}votte, Cemgil - Nonnegative matrix factorizations as probabilistic inference in compos.pdf:pdf},
issn = {22195491},
journal = {European Signal Processing Conference},
number = {Eusipco},
pages = {1913--1917},
title = {{Nonnegative matrix factorizations as probabilistic inference in composite models}},
year = {2009}
}
@article{Kuhn2005,
abstract = {A stochastic approximation version of EM for maximum likelihood estimation of a wide class of nonlinear mixed effects models is proposed. The main advantage of this algorithm is its ability to provide an estimator close to the MLE in very few iterations. The likelihood of the observations as well as the Fisher Information matrix can also be estimated by stochastic approximations. Numerical experiments allow to highlight the very good performances of the proposed method. ?? 2004 Elsevier B.V. All rights reserved.},
author = {Kuhn, E. and Lavielle, M.},
doi = {10.1016/j.csda.2004.07.002},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Computational Statistics and Data Analysis - 2005 - Kuhn, Lavielle - Maximum likelihood estimation in nonlinear mixed effects models.pdf:pdf},
isbn = {0167-9473},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {EM algorithm,Maximum likelihood estimation,Mixed effects model,Nonlinear model,SAEM algorithm},
number = {4},
pages = {1020--1038},
title = {{Maximum likelihood estimation in nonlinear mixed effects models}},
volume = {49},
year = {2005}
}
@article{Tan2013,
abstract = {This paper addresses the estimation of the latent dimensionality in nonnegative matrix factorization (NMF) with the $\beta$-divergence. The $\beta$-divergence is a family of cost functions that includes the squared euclidean distance, Kullback-Leibler (KL) and Itakura-Saito (IS) divergences as special cases. Learning the model order is important as it is necessary to strike the right balance between data fidelity and overfitting. We propose a Bayesian model based on automatic relevance determination (ARD) in which the columns of the dictionary matrix and the rows of the activation matrix are tied together through a common scale parameter in their prior. A family of majorization-minimization (MM) algorithms is proposed for maximum a posteriori (MAP) estimation. A subset of scale parameters is driven to a small lower bound in the course of inference, with the effect of pruning the corresponding spurious components. We demonstrate the efficacy and robustness of our algorithms by performing extensive experiments on synthetic data, the swimmer dataset, a music decomposition example, and a stock price prediction task.},
archivePrefix = {arXiv},
arxivId = {arXiv:1111.6085v3},
author = {Tan, Vincent Y F and F{\'{e}}votte, C{\'{e}}dric},
doi = {10.1109/TPAMI.2012.240},
eprint = {arXiv:1111.6085v3},
file = {:home/alumbreras/Documentos/Mendeley/Papers/IEEE Transactions on Pattern Analysis and Machine Intelligence - 2013 - Tan, F{\'{e}}votte - Automatic relevance determination in nonnegative.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Nonnegative matrix factorization,automatic relevance determination,group-sparsity,majorization-minimization,model order selection},
number = {7},
pages = {1592--1605},
pmid = {23681989},
title = {{Automatic relevance determination in nonnegative matrix factorization with the beta-divergence}},
volume = {35},
year = {2013}
}
@article{Honkela2010,
abstract = {Variational Bayesian (VB) methods are typically only applied to models in the conjugate-exponential family using the variational Bayesian expectation maximisation (VB EM) algorithm or one of its variants. In this paper we present an efficient algorithm for applying VB to more general models. The method is based on specifying the functional form of the approximation, such as multivariate Gaussian. The parameters of the approximation are optimised using a conjugate gradient algorithm that utilises the Riemannian geometry of the space of the approximations. This leads to a very efficient algorithm for suitably structured approximations. It is shown empirically that the proposed method is comparable or superior in efficiency to the VB EM in a case where both are applicable. We also apply the algorithm to learning a nonlinear state-space model and a nonlinear factor analysis model for which the VB EM is not applicable. For these models, the proposed algorithm outperforms alternative gradient-based methods by a significant margin.},
annote = {En lugar de mean fields, se puede aproximar con una sola distribucion y usar optimizacion (ej Gradient Descent) en lugar de el EM del VBEM clasico explicado en Bishop},
author = {Honkela, Antti and Raiko, Tapani and Kuusela, Mikael and Tornio, Matti and Karhunen, Juha},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Journal of Machine Learning Research - 2010 - Honkela et al. - Approximate Riemannian conjugate gradient learning for fixed-form variati.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {computational,information theoretic learning with statistics,learning,statistics {\&} optimisation,theory {\&} algorithms},
number = {5},
pages = {3235--3268},
title = {{Approximate Riemannian conjugate gradient learning for fixed-form variational Bayes}},
url = {http://eprints.pascal-network.org/archive/00007751/},
volume = {11},
year = {2010}
}
@inproceedings{Jamali2011,
author = {Jamali, Mohsen},
booktitle = {Proceedings of the Twenty-Second international joint conference on Artificial Intelligence - IJCAI '11},
doi = {10.5591/978-1-57735-516-8/IJCAI11-440},
file = {:home/alumbreras/Documentos/Mendeley/Papers//Proceedings of the Twenty-Second international joint conference on Artificial Intelligence - IJCAI '11 - 2011 - Jamali - A Transitivity.pdf:pdf},
keywords = {Best Paper Track},
pages = {2644--2649},
title = {{A Transitivity Aware Matrix Factorization Model for Recommendation in Social Networks}},
url = {http://www.aaai.org/ocs/index.php/IJCAI/IJCAI11/paper/viewPDFInterstitial/2973/3649},
year = {2011}
}
@article{Berger2012,
abstract = {Elimination of nuisance parameters is a central problem in statistical inference and has been formally studied in virtually all approaches to inference. Perhaps the least studied approach is elimination of nuisance parameters through integration, in the sense that this is viewed as an almost incidental byproduct of Bayesian analysis and is hence not something which is deemed to require separate study. There is, however, considerable value in considering integrated likelihood on its own, especially versions arising from default or noninformative priors. In this paper, we review such common integrated likelihoods and discuss their strengths and weaknesses relative to other methods.},
author = {Berger, James O and Liseo, Brunero and Wolpert, Robert L},
doi = {10.1214/ss/1009211804},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Statistical Science. A Review Journal of the Institute of Mathematical Statistics - 2012 - Berger, Liseo, Wolpert - Integrated Likelihoo.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science. A Review Journal of the Institute of Mathematical Statistics},
keywords = {and phrases,file likelihood,marginal likelihood,nuisance parameters,pro-,reference priors},
number = {1},
pages = {1--22},
title = {{Integrated Likelihood Methods for Eliminating Nuisance Parameters}},
url = {http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body{\&}id=pdf{\_}1{\&}handle=euclid.ss/1009211804$\backslash$npapers2://publication/doi/10.1214/ss/1009211803},
volume = {14},
year = {2012}
}
@article{Rezende2015,
abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
archivePrefix = {arXiv},
arxivId = {arXiv:1505.05770v4},
author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
eprint = {arXiv:1505.05770v4},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Proceedings of the 32nd International Conference on Machine Learning - 2015 - Rezende, Mohamed - Variational Inference with Normalizing.pdf:pdf},
isbn = {1505.05770},
journal = {Proceedings of the 32nd International Conference on Machine Learning},
pages = {1530--1538},
title = {{Variational Inference with Normalizing Flows}},
url = {http://arxiv.org/abs/1505.05770},
volume = {37},
year = {2015}
}
@article{Paisley2009,
abstract = {We propose a nonparametric extension to the factor analysis problem using a beta process prior. This beta process factor analysis (BP-FA) model allows for a dataset to be decomposed into a linear combination of a sparse set of factors, providing information on the underlying structure of the observations. As with the Dirichlet process, the beta process is a fully Bayesian conjugate prior, which allows for analytical posterior calculation and straightforward inference. We derive a varia-tional Bayes inference algorithm and demonstrate the model on the MNIST digits and HGDP-CEPH cell line panel datasets.},
author = {Paisley, John and Carin, Lawrence},
doi = {10.1145/1553374.1553474},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Proceedings of the 26th International Conference on Machine Learning - 2009 - Paisley, Carin - Nonparametric Factor Analysis with Beta P.pdf:pdf},
isbn = {9781605585161},
journal = {Proceedings of the 26th International Conference on Machine Learning},
pages = {1--8},
title = {{Nonparametric Factor Analysis with Beta Process Priors}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553474},
year = {2009}
}
@article{Rezende2014,
abstract = {Abstract We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep , directed generative models , endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition ... $\backslash$n},
archivePrefix = {arXiv},
arxivId = {arXiv:1401.4082v3},
author = {Rezende, D J and Mohamed, S and Wierstra, D},
eprint = {arXiv:1401.4082v3},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Proceedings of The 31st {\ldots} - 2014 - Rezende, Mohamed, Wierstra - Stochastic backpropagation and approximate inference in deep generativ.pdf:pdf},
isbn = {9781634393973},
journal = {Proceedings of The 31st {\ldots}},
pages = {1278--1286},
title = {{Stochastic backpropagation and approximate inference in deep generative models}},
url = {http://jmlr.org/proceedings/papers/v32/rezende14.html{\%}5Cnpapers3://publication/uuid/F2747569-7719-4EAC-A5A7-9ECA9D6A8FE6},
volume = {32},
year = {2014}
}
@article{Hoffman2013,
abstract = {Abstract: We develop stochastic variational inference , a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and ...},
archivePrefix = {arXiv},
arxivId = {1206.7051},
author = {Hoffman, Matthew D},
doi = {citeulike-article-id:10852147},
eprint = {1206.7051},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Journal of Machine Learning Research - 2013 - Hoffman - Stochastic Variational Inference.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
pages = {1303--1347},
title = {{Stochastic Variational Inference}},
url = {http://arxiv.org/abs/1206.7051$\backslash$npapers2://publication/uuid/D5737928-F59F-43E3-8ADE-53F1831AA866},
volume = {14},
year = {2013}
}
@article{Azoury2001,
abstract = {We consider on-line density estimation with a parameterized density from the exponential family. The on-line algorithm receives one example at a time and maintains a parameter that is essentially an average of the past examples. After receiving an example the algorithm incurs a loss which is the negative log-likelihood of the example w.r.t. the past parameter of the algorithm. An off-line algorithm can choose the best parameter based on all the examples. We prove bounds on the additional total loss of the on-line algorithm over the total loss of the off-line algorithm. These relative loss bounds hold for an arbitrary sequence of examples. The goal is to design algorithms with the best possible relative loss bounds. We use a certain divergence to derive and analyze the algorithms. This divergence is a relative entropy between two exponential distributions.},
archivePrefix = {arXiv},
arxivId = {1301.6677},
author = {Azoury, Katy S. and Warmuth, M. K.},
doi = {10.1023/A:1010896012157},
eprint = {1301.6677},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Machine Learning - 2001 - Azoury, Warmuth - Relative loss bounds for on-line density estimation with the exponential family of distribut.pdf:pdf},
isbn = {1-55860-614-9},
issn = {08856125},
journal = {Machine Learning},
keywords = {Bregman divergences,Exponential family of distributions,Linear regression,On-line algorithms,Relative loss bounds,Worst-case loss bounds},
number = {3},
pages = {211--246},
title = {{Relative loss bounds for on-line density estimation with the exponential family of distributions}},
volume = {43},
year = {2001}
}
@article{Renkens2017,
author = {Renkens, Vincent and Van hamme, Hugo},
doi = {10.1016/j.sigpro.2016.09.009},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Signal Processing - 2017 - Renkens, Van hamme - Automatic relevance determination for nonnegative dictionary learning in the gamma-Poiss.pdf:pdf},
issn = {01651684},
journal = {Signal Processing},
keywords = {Model order selection,Nonnegative matrix factorization,Pattern recognition,Variational EM},
number = {July 2016},
pages = {121--133},
publisher = {Elsevier},
title = {{Automatic relevance determination for nonnegative dictionary learning in the gamma-Poisson model}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0165168416302353},
volume = {132},
year = {2017}
}
@article{Chib1995,
abstract = {In the context of Bayes estimation via Gibbs sampling, with or without data augmentation, a simple approach is developed for computing the marginal density of the sample data (marginal likelihood) given parameter draws from the posterior distribution. Consequently, Bayes factors for model comparisons can be routinely computed as a by-product of the simulation. Hitherto, this calculation has proved extremely challenging. Our approach exploits the fact that the marginal density can be expressed as the prior times the likelihood function over the posterior density. This simple identity holds for any parameter value. An estimate of the posterior density is shown to be available if all complete conditional densities used in the Gibbs sampler have closed-form expressions. To improve accuracy, the posterior density is estimated at a high density point, and the numerical standard error of resulting estimate is derived. The ideas are applied to probit regression and finite mixture models.},
author = {Chib, Siddhartha},
doi = {10.2307/2291521},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Journal of the American Statistical Association - 2008 - Url, Archive, Archive - Marginal Likelihood from the Gibbs Output Siddhartha.pdf:pdf},
isbn = {0162-1459},
issn = {{\textless}null{\textgreater}},
journal = {Journal of the American Statistical Association},
keywords = {1994,bayes factor,carlo,equivalently,estimation of normalizing constant,finite mixture models,likelihood,linear regression,markov chain monte,markov mixture model,multivariate density estimation,numerical standard error,probit regression,reduced conditional density,showed that the marginal},
number = {432},
pages = {1313--1321},
pmid = {12510683},
title = {{Marginal Likelihood from the Gibbs Output}},
url = {http://www.jstor.org/stable/2291521$\backslash$npapers2://publication/uuid/0B20DBD5-06CE-4DC9-91B8-C22D660DBD2A},
volume = {90},
year = {1995}
}
@article{Ma2008,
abstract = {Data sparsity, scalability and prediction quality have been recognized as the three most crucial challenges that every collaborative filtering algorithm or recommender system confronts. Many existing approaches to recommender systems can neither handle very large datasets nor easily deal with users who have made very few ratings or even none at all. Moreover, traditional recommender systems assume that all the users are independent and identically distributed; this assumption ignores the social interactions or connections among users. In view of the exponential growth of information generated by online social networks, social network analysis is becoming important for many Web applications. Following the intuition that a person's social network will affect personal behaviors on the Web, this paper proposes a factor analysis approach based on probabilistic matrix factorization to solve the data sparsity and poor prediction accuracy problems by employing both users' social network information and rating records. The complexity analysis indicates that our approach can be applied to very large datasets since it scales linearly with the number of observations, while the experimental results shows that our method performs much better than the state-of-the-art approaches, especially in the circumstance that users have made few or no ratings.},
author = {Ma, Hao and Yang, Haixuan and Lyu, Michael R and King, Irwin},
doi = {10.1145/1458082.1458205},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Proceeding of the 17th ACM conference on Information and knowledge management - 2008 - Ma et al. - SoRec Social Recommendation Using Pr.pdf:pdf},
isbn = {9781595939913},
journal = {Proceeding of the 17th ACM conference on Information and knowledge management},
keywords = {Collaborative Filtering,Matrix Factorization,Recommender Systems,Social Rec- ommendation},
pages = {0--9},
title = {{SoRec : Social Recommendation Using Probabilistic Matrix Factorization}},
url = {http://portal.acm.org/citation.cfm?id=1458205},
volume = {08pages},
year = {2008}
}
@article{Salimans2013,
abstract = {We propose a general algorithm for approximating nonstandard Bayesian posterior distributions. The algorithm minimizes the Kullback-Leibler divergence of an approximating distribution to the intractable posterior distribution. Our method can be used to approximate any posterior distribution, provided that it is given in closed form up to the proportionality constant. The approximation can be any distribution in the exponential family or any mixture of such distributions, which means that it can be made arbitrarily precise. Several examples illustrate the speed and accuracy of our approximation method in practice.},
archivePrefix = {arXiv},
arxivId = {1206.6679},
author = {Salimans, Tim and Knowles, David A.},
doi = {10.1214/13-BA858},
eprint = {1206.6679},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Bayesian Analysis - 2013 - Salimans, Knowles - Fixed-form variational posterior approximation through stochastic linear regression.pdf:pdf},
isbn = {1401.2135},
issn = {19360975},
journal = {Bayesian Analysis},
keywords = {Approximate inference,Stochastic approximation,Variational bayes},
number = {4},
pages = {837--882},
title = {{Fixed-form variational posterior approximation through stochastic linear regression}},
volume = {8},
year = {2013}
}
@inproceedings{Mohamed2007,
author = {Mohamed, Shakir and Heller, Katherine and Ghahramani, Zoubin},
booktitle = {Advances in Neural Information Processing Systems},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Advances in Neural Information Processing Systems - 2007 - Mohamed, Heller, Ghahramani - Bayesian Exponential Family PCA.pdf:pdf},
title = {{Bayesian Exponential Family PCA}},
year = {2007}
}
@article{Zhou2013,
author = {Zhou, Mingyuan},
file = {:home/alumbreras/Documentos/Mendeley/Papers/PhD Thesis - 2013 - Zhou - Nonparametric Bayesian Dictionary Learning and Count and Mixture Modeling.pdf:pdf},
journal = {PhD Thesis},
title = {{Nonparametric Bayesian Dictionary Learning and Count and Mixture Modeling}},
year = {2013}
}
@article{Beal2003,
abstract = {We present an efficient procedure for estimating the marginal likelihood of probabilistic models with latent variables or incomplete data. This method constructs and optimises a lower bound on the marginal likelihood using variational calculus, resulting in an iterative algorithm which generalises the EM algorithm by maintaining posterior distributions over both latent variables and parameters. We define the family of conjugate-exponential models—which includes finite mixtures of exponential family models, factor analysis, hidden Markov models, linear state-space models, and other models of interest—for which this bound on the marginal likelihood can be computed very simply through a modification of the standard EM algorithm. In particular, we focus on applying these bounds to the problem of scoring discrete directed graphical model structures (Bayesian networks). Extensive simulations comparing the variational bounds to the usual approach based on the Bayesian Information Criterion (BIC) and to a sampling-based gold standard method known as Annealed Importance Sampling (AIS) show that variational bounds substantially outperform BIC in finding the correct model structure at relatively little computational cost, while approaching the performance of the much more costly AIS procedure. Using AIS allows us to provide the first serious case study of the tightness of variational bounds. We also analyse the perfomance of AIS through a variety of criteria, and outline directions in which this work can be extended.},
author = {Beal, Matthew J and Ghahramani, Zoubin},
doi = {10.1152/jn.00664.2010},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Unknown - 2003 - Computational, Unit - The Variational Bayesian EM Algorithm for Incomplete Data with Application to Scoring Graphical M.pdf:pdf},
isbn = {0198526156},
issn = {15221598},
journal = {Bayesian statistics 7: proceedings of the seventh Valencia International Meeting, June 2-6, 2002},
keywords = {annealed importance sampling,bayes factors,graphical,latent variables,marginal likelihood,models,structure scoring,variational methods},
pages = {453},
pmid = {20926606},
title = {{The variational Bayesian EM algorithm for incomplete data: with application to scoring graphical model structures}},
url = {papers2://publication/uuid/CA675505-6092-4BE3-8200-EDF80F490DB9},
year = {2003}
}
@article{Ivek2014,
abstract = {Probabilistic interpretations of standard divergences used by divergence-based nonnegative matrix factorization (NMF) will be established. One advantage of using probabilistic generalizations of NMF is that well-developed probabilistic learning methods can be used straightforwardly as NMF decomposition algorithms. They also allow embedding of richer prior knowledge in form of probabilistic structural constraints in the model in order to obtain better or application-specific decompositions.},
author = {Ivek, Ivan},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Ztel.Fer.Unizg.Hr - 2014 - Ivek - Probabilistic Formulations of Nonnegative Matrix Factorization.pdf:pdf},
journal = {Ztel.Fer.Unizg.Hr},
pages = {1--11},
title = {{Probabilistic Formulations of Nonnegative Matrix Factorization}},
url = {http://ztel.fer.unizg.hr/{\_}download/repository/KDI,{\_}Ivan{\_}Ivek.pdf},
year = {2014}
}
@article{Cichocki2010,
abstract = {In this paper, we extend and overview wide families of Alpha-, Beta- and Gamma-divergences and discuss their fundamental properties. In literature usually only one single asymmetric (Alpha, Beta or Gamma) divergence is considered. We show in this paper that there exist families of such divergences with the same consistent properties. Moreover, we establish links and correspondences among these divergences by applying suitable nonlinear transformations. For example, we can generate the Beta-divergences directly from Alpha-divergences and vice versa. Furthermore, we show that a new wide class of Gamma-divergences can be generated not only from the family of Beta-divergences but also from a family of Alpha-divergences. The paper bridges these divergences and shows also their links to Tsallis and Runhbox voidb@x  group let unhbox voidb@x setbox @tempboxa hbox {\{}eglobal mathchardef accent@spacefactor spacefactor {\}}accent 19 eegroup spacefactor accent@spacefactor nyi entropies. Most of these divergences have a natural information theoretic interpretation.},
author = {Cichocki, Andrzej and ichi Amari, Shun},
doi = {10.3390/e12061532},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Unknown - 2010 - Cichocki, Amari - Families of Alpha- Beta- and Gamma- Divergences Flexible and Robust Measures of Similarities.pdf:pdf},
issn = {10994300},
journal = {Entropy},
keywords = {Csisz??r-Morimoto and Bregman divergences,Extended Itakura-Saito like divergences,Generalized divergences,Similarity measures,Tsallis and R??nyi entropies},
number = {6},
pages = {1532--1568},
title = {{Families of alpha- beta- and gamma- divergences: Flexible and robust measures of Similarities}},
volume = {12},
year = {2010}
}
@techreport{ThomasL.Griffiths2005b,
abstract = {We define a probability distribution over equivalence classes of binary matrices with a finite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially infinite array of features. We identify a simple generative process that results in the same distribution over equivalence classes, which we call the Indian buffet process. We illustrate the use of this distribution as a prior in an infinite latent feature model, deriving a Markov chain Monte Carlo algorithm for inference in this model and applying the algorithm to an image dataset.},
author = {{Thomas L. Griffiths}, Zoubin Ghahramani},
booktitle = {Technical Report},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Advances in Neural Information Processing Systems 18 - 2005 - Thomas L. Griffiths - Infinite Latent Feature Models and the Indian Buf(3).pdf:pdf},
isbn = {9780262232531},
issn = {1049-5258},
pages = {475--482},
title = {{Infinite Latent Feature Models and the Indian Buffet Process}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.60.3951},
year = {2005}
}
@article{Cemgil2009,
author = {Cemgil, Ali Taylan},
doi = {10.1155/2009/785152},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Computational Intelligence and Neuroscience - 2009 - Cemgil - Bayesian Inference for Nonnegative Matrix Factorisation Models.pdf:pdf},
journal = {Computational Intelligence and Neuroscience},
pages = {1--17},
title = {{Bayesian Inference for Nonnegative Matrix Factorisation Models}},
volume = {2009},
year = {2009}
}
@article{ThomasL.Griffiths2005a,
abstract = {We define a probability distribution over equivalence classes of binary matrices with a finite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially infinite array of features. We identify a simple generative process that results in the same distribution over equivalence classes, which we call the Indian buffet process. We illustrate the use of this distribution as a prior in an infinite latent feature model, deriving a Markov chain Monte Carlo algorithm for inference in this model and applying the algorithm to an image dataset.},
author = {{Thomas L. Griffiths}, Zoubin Ghahramani},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Advances in Neural Information Processing Systems 18 - 2005 - Thomas L. Griffiths - Infinite Latent Feature Models and the Indian Buf(2).pdf:pdf},
isbn = {9780262232531},
issn = {1049-5258},
journal = {Advances in Neural Information Processing Systems 18},
pages = {475--482},
title = {{Infinite Latent Feature Models and the Indian Buffet Process}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.60.3951},
year = {2005}
}
@article{Barlow1989,
author = {Barlow, H. B.},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Unsupervised Learning - 1989 - Barlow - Unsupervised Learning.pdf:pdf},
journal = {Unsupervised Learning},
pages = {295--311},
title = {{Unsupervised Learning}},
volume = {1},
year = {1989}
}
@article{Zhou2012a,
abstract = {Nonparametric Bayesian methods are considered for recovery of imagery based upon compressive, incomplete, and/or noisy measurements. A truncated beta-Bernoulli process is employed to infer an appropriate dictionary for the data under test and also for image recovery. In the context of compressive sensing, significant improvements in image recovery are manifested using learned dictionaries, relative to using standard orthonormal image expansions. The compressive-measurement projections are also optimized for the learned dictionary. Additionally, we consider simpler (incomplete) measurements, defined by measuring a subset of image pixels, uniformly selected at random. Spatial interrelationships within imagery are exploited through use of the Dirichlet and probit stick-breaking processes. Several example results are presented, with comparisons to other methods in the literature.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Zhou, Mingyuan and Chen, Haojun and Paisley, John and Ren, Lu and Li, Lingbo and Xing, Zhengming and Dunson, David and Sapiro, Guillermo and Carin, Lawrence},
doi = {10.1109/TIP.2011.2160072},
eprint = {NIHMS150003},
file = {:home/alumbreras/Documentos/Mendeley/Papers/IEEE Transactions on Image Processing - 2012 - Zhou et al. - Nonparametric bayesian dictionary learning for analysis of noisy and incomp.pdf:pdf},
isbn = {1057-7149},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Bayesian nonparametrics,compressive sensing,dictionary learning,factor analysis,image denoising,image interpolation,sparse coding},
number = {1},
pages = {130--144},
pmid = {21693421},
title = {{Nonparametric bayesian dictionary learning for analysis of noisy and incomplete images}},
volume = {21},
year = {2012}
}
@article{Agarwal2010,
abstract = {We propose fLDA, a novel matrix factorization method to predict ratings in recommender system applications where a “bag-of-words” representation for item meta-data is natural. Such scenarios are commonplace in web applications like content recommendation, ad targeting and web search where items are articles, ads and web pages respectively. Because of data sparseness, regularization is key to good predictive accuracy. Our method works by regularizing both user and item factors simultaneously through user features and the bag of words associated with each item. Specifically, each word in an item is associated with a discrete latent factor often referred to as the topic of the word; item topics are obtained by averaging topics across all words in an item. Then, user rating on an item is modeled as user's affinity to the item's topics where user affinity to topics (user factors) and topic assignments to words in items (item factors) are learned jointly in a supervised fashion. To avoid overfitting, user and item factors are regularized through Gaussian linear regression and Latent Dirichlet Allocation (LDA) priors respectively. We show our model is accurate, interpretable and handles both cold-start and warm-start scenarios seamlessly through a single model. The efficacy of our method is illustrated on benchmark datasets and a new dataset from Yahoo! Buzz where fLDA provides superior predictive accuracy in cold-start scenarios and is comparable to state-ofthe-art methods in warm-start scenarios. As a by-product, fLDA also identifies interesting topics that explains useritem interactions. Our method also generalizes a recently proposed technique called supervised LDA (sLDA) to collaborative filtering applications. While sLDA estimates item topic vectors in a supervised fashion for a single regression, fLDA incorporates multiple regressions (one for each user) in estimating the item factors.},
author = {Agarwal, Deepak and Chen, Bee-Chung},
doi = {10.1145/1718487.1718499},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Proceedings of the third ACM international conference on Web search and data mining (WSDM) - 2010 - Agarwal, Chen - fLDA Matrix Factoriz.pdf:pdf},
isbn = {9781605588896},
issn = {160558889X},
journal = {Proceedings of the third ACM international conference on Web search and data mining (WSDM)},
keywords = {bayesian hierarchical model,collaborative filtering,graphical model,latent factor model,recommender systems,supervised topic model},
pages = {91},
title = {{fLDA: Matrix Factorization through Latent Dirichlet Allocation}},
url = {http://dl.acm.org/citation.cfm?id=1718487.1718499},
year = {2010}
}
@article{SoulMachine2015,
author = {SoulMachine},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Unknown - 2015 - SoulMachine - MachineLearning CheatSheet.pdf:pdf},
pages = {135},
title = {{MachineLearning CheatSheet}},
year = {2015}
}
@article{Titsias2008adraft,
author = {Titsias, Michalis K},
file = {:home/alumbreras/Documentos/Mendeley/Papers/NIPS draft - 2008 - Titsias - The Infinite Gamma-Poisson Feature Model - draft.pdf:pdf},
journal = {NIPS draft},
keywords = {GaP,Gamma-Poisson,non-negative integer valued matrices,nonparametric Bayes,unlabelled images},
pages = {1--8},
title = {{The Infinite Gamma-Poisson Feature Model - draft}},
year = {2008}
}
@article{Griffiths2011,
abstract = {The Indian buffet process is a stochastic process defining a probability distribution over equiva- lence classes of sparse binary matrices with a finite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilisticmodels that represent objects using a potentially infinite array of features, or that involve bipartite graphs in which the size of at least one class of nodes is unknown. We give a detailed derivation of this distribution, and illustrate its use as a prior in an infinite latent featuremodel. We then review recent applications of the Indian buffet process in machine learning, discuss its extensions, and summarize its connections to other stochastic processes.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Griffiths, Thomas L. and Ghahramani, Zoubin},
doi = {10.1016/j.biotechadv.2011.08.021.Secreted},
eprint = {NIHMS150003},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Journal ofMachine Learning Research - 2011 - Griffiths, Ghahramani - The Indian Buffet Process An Introduction and Review.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal ofMachine Learning Research},
keywords = {beta process,chinese,exchangeable distributions,latent variable models,markov chain monte carlo,nonparametric bayes,restaurant processes,sparse binary matrices},
pages = {1185--1224},
pmid = {290096100001},
title = {{The Indian Buffet Process: An Introduction and Review}},
volume = {12},
year = {2011}
}
@article{Ranganath2014b,
author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David M},
file = {:home/alumbreras/Documentos/Mendeley/Papers/AISTATS - 2014 - Ranganath, Gerrish, Blei - Black Box Variational Inference ( Extra Materials ).pdf:pdf},
journal = {AISTATS},
title = {{Black Box Variational Inference ( Extra Materials )}},
year = {2014}
}
@article{Lavenu2012,
abstract = {saemix, an R version of the SAEM algorithm for parameter estimation in nonlinear mixed effect models},
author = {Lavenu, Audrey and Comets, Emmanuelle and Lavielle, Marc},
file = {:home/alumbreras/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lavenu, Comets, Lavielle - 2012 - saemix, an R version of the SAEM algorithm for parameter estimation in nonlinear mixed effect models.pdf:pdf},
journal = {1{\`{e}}res Rencontres R},
keywords = {Nonlinear mixed effect models,R package,SAEM algorithm,longitudinal data,parameter estimation,pharmacodynamics,pharmacokinetics},
pages = {1--6},
title = {{saemix, an R version of the SAEM algorithm for parameter estimation in nonlinear mixed effect models}},
url = {https://hal-univ-rennes1.archives-ouvertes.fr/hal-00717539},
year = {2012}
}
@article{Broderick2012,
abstract = {The beta-Bernoulli process provides a Bayesian nonparametric prior for models involving collections of binary-valued features. A draw from the beta process yields an infinite collection of probabilities in the unit interval, and a draw from the Bernoulli process turns these into binary-valued features. Recent work has provided stick-breaking representations for the beta process analogous to the well-known stick-breaking representation for the Dirichlet process. We derive one such stick-breaking representation directly from the characterization of the beta process as a completely random measure. This approach motivates a three-parameter generalization of the beta process, and we study the power laws that can be obtained from this generalized beta process. We present a posterior inference algorithm for the beta-Bernoulli process that exploits the stick-breaking representation, and we present experimental results for a discrete factor-analysis model.},
archivePrefix = {arXiv},
arxivId = {1106.0539},
author = {Broderick, Tamara and Jordan, Michael I. and Pitman, Jim},
doi = {10.1214/12-BA715},
eprint = {1106.0539},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Bayesian Analysis - 2012 - Broderick, Jordan, Pitman - Beta processes, stick-breaking and power laws.pdf:pdf},
issn = {19360975},
journal = {Bayesian Analysis},
keywords = {Beta process,Power law,Stick-breaking},
number = {2},
pages = {439--476},
title = {{Beta processes, stick-breaking and power laws}},
volume = {7},
year = {2012}
}
@phdthesis{TadayukiMiller2011,
author = {{Tadayuki Miller}, Kurt},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Unknown - 2011 - Tadayuki Miller - Bayesian Nonparametric Latent Feature Models.pdf:pdf},
school = {University of California, Berkeley},
title = {{Bayesian Nonparametric Latent Feature Models}},
year = {2011}
}
@article{Titsias2008,
abstract = {We present a probability distribution over non-negative integer valued matrices with possibly an infinite number of columns. We also derive a stochastic process that reproduces this distribution over equivalence classes. This model can play the role of the prior in nonparametric Bayesian learning scenarios where multiple latent features are associated with the observed data and each feature can have multiple appearances or occurrences within each data point. Such data arise naturally when learning visual object recognition systems from unlabelled images. Together with the nonparametric prior we consider a likelihood model that explains the visual appearance and location of local image patches. Inference with this model is carried out using a Markov chain Monte Carlo algorithm.},
author = {Titsias, Michalis K},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Advances in Neural Information Processing Systems (NIPS) 20 - 2008 - Titsias - The Infinite Gamma-Poisson Feature Model.pdf:pdf},
isbn = {160560352X},
journal = {Advances in Neural Information Processing Systems (NIPS) 20},
keywords = {GaP,Gamma-Poisson,non-negative integer valued matrices,nonparametric Bayes,unlabelled images},
title = {{The Infinite Gamma-Poisson Feature Model}},
year = {2008}
}
@article{Agakov2004,
abstract = {Variational methods have proved popular and effective for inference and learning in intractable graphical models. An attractive feature of the approaches based on the Kullback-Leibler divergence is a rigorous lower bound on the normalization constants in undirected models. In the suggested work we explore the idea of using auxiliary variables to improve on the lower bound of standard mean field methods. Our approach forms a more powerful class of approximations than any structured mean field technique. Furthermore, the existing lower bounds of the variational mixture models could be seen as computationally expensive special cases of our method. A byproduct of our work is an efficient way to calculate a set of mixture coefficients for any set of tractable distributions that principally improves on a flat combination.},
author = {Agakov, FV and Barber, D},
doi = {10.1007/978-3-540-30499-9_86},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Neural Information Processing - 2004 - Agakov, Barber - An auxiliary variational method.pdf:pdf},
isbn = {9783540239314},
issn = {03029743},
journal = {Neural Information Processing},
number = {2},
pages = {1--10},
title = {{An auxiliary variational method}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-30499-9{\_}86},
year = {2004}
}
@article{Kuhn2004,
abstract = {The fields of machine learning and mathematical programming are increasingly intertwined. Op-timization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning re-searchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semi-definite, and semi-infinite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical pro-gramming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed im-provements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for specific classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general pur-pose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms.},
archivePrefix = {arXiv},
arxivId = {math/0601771},
author = {Kuhn, Estelle and Lavielle, Marc},
doi = {10.1051/ps},
eprint = {0601771},
file = {:home/alumbreras/Documentos/Mendeley/Papers/ESAIM Probability and Statistics - 2004 - Kuhn, Lavielle - Coupling a stochastic approximation of EM with an MCMC version of EM with an.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {ESAIM: Probability and Statistics},
keywords = {SAEM,convex optimization,machine learning,mathematical programming},
mendeley-tags = {SAEM},
pages = {115--131},
primaryClass = {math},
title = {{Coupling a stochastic approximation of EM with an MCMC version of EM with an MCMC procedure}},
volume = {8},
year = {2004}
}
@incollection{Singh2008,
author = {Singh, Ajit P and Gordon, Geoffrey J},
booktitle = {LNCS},
file = {:home/alumbreras/Documentos/Mendeley/Papers/LNCS - 2008 - Singh, Gordon - A Unified View of Matrix Factorization Models.pdf:pdf},
pages = {358--373},
title = {{A Unified View of Matrix Factorization Models}},
year = {2008}
}
@article{Montanari2014,
abstract = {Principal component analysis (PCA) aims at estimating the direction of maximal variability of a high-dimensional dataset. A natural question is: does this task become easier, and estimation more accurate, when we exploit additional knowledge on the principal vector? We study the case in which the principal vector is known to lie in the positive orthant. Similar constraints arise in a number of applications, ranging from analysis of gene expression data to spike sorting in neural signal processing. In the unconstrained case, the estimation performances of PCA has been precisely characterized using random matrix theory, under a statistical model known as the `spiked model.' It is known that the estimation error undergoes a phase transition as the signal-to-noise ratio crosses a certain threshold. Unfortunately, tools from random matrix theory have no bearing on the constrained problem. Despite this challenge, we develop an analogous characterization in the constrained case, within a one-spike model. In particular: {\$}(i){\$}{\~{}}We prove that the estimation error undergoes a similar phase transition, albeit at a different threshold in signal-to-noise ratio that we determine exactly; {\$}(ii){\$}{\~{}}We prove that --unlike in the unconstrained case-- estimation error depends on the spike vector, and characterize the least favorable vectors; {\$}(iii){\$}{\~{}}We show that a non-negative principal component can be approximately computed --under the spiked model-- in nearly linear time. This despite the fact that the problem is non-convex and, in general, NP-hard to solve exactly.},
archivePrefix = {arXiv},
arxivId = {1406.4775},
author = {Montanari, Andrea and Richard, Emile},
eprint = {1406.4775},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Unknown - 2014 - Montanari, Richard - Non-negative Principal Component Analysis Message Passing Algorithms and Sharp Asymptotics.pdf:pdf},
number = {1},
title = {{Non-negative Principal Component Analysis: Message Passing Algorithms and Sharp Asymptotics}},
url = {http://arxiv.org/abs/1406.4775},
volume = {2014},
year = {2014}
}
@article{Zhou2009,
abstract = {Non-parametric Bayesian techniques are considered for learning dictionaries for sparse image representations, with applications in denoising, inpainting and com- pressive sensing (CS). The beta process is employed as a prior for learning the dictionary, and this non-parametric method naturally infers an appropriate dic- tionary size. The Dirichlet process and a probit stick-breaking process are also considered to exploit structure within an image. The proposed method can learn a sparse dictionary in situ; training images may be exploited if available, but they are not required. Further, the noise variance need not be known, and can be non- stationary. Another virtue of the proposed method is that sequential inference can be readily employed, thereby allowing scaling to large images. Several example results are presented, using both Gibbs and variational Bayesian inference, with comparisons to other state-of-the-art approaches.},
author = {Zhou, Mingyuan and Chen, Haojun and Paisley, John and Ren, Lu and Sapiro, Guillermo and Carin, Lawrence},
doi = {citeulike-article-id:6110001},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Neural Information Processing Systems - 2009 - Zhou et al. - Non-Parametric Bayesian Dictionary Learning for Sparse Image Representation.pdf:pdf},
isbn = {9781615679119},
journal = {Neural Information Processing Systems},
pages = {1--9},
title = {{Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations}},
year = {2009}
}
@article{Gershman2012,
author = {Gershman, Samuel J. and Blei, David M.},
doi = {10.1016/j.jmp.2011.08.004},
file = {:home/alumbreras/Documentos/Mendeley/Papers//Journal of Mathematical Psychology - 2012 - Gershman, Blei - A tutorial on Bayesian nonparametric models.pdf:pdf},
issn = {00222496},
journal = {Journal of Mathematical Psychology},
month = {feb},
number = {1},
pages = {1--12},
publisher = {Elsevier Inc.},
title = {{A tutorial on Bayesian nonparametric models}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S002224961100071X},
volume = {56},
year = {2012}
}
@article{Allassonniere2010,
abstract = {The problem of the definition and the estimation of generative models based on deformable templates from raw data is of particular importance for modelling non aligned data affected by various types of geometrical variability. This is especially true in shape modelling in the computer vision community or in probabilistic atlas building for Computational Anatomy (CA). A first coherent statistical framework modelling the geometrical variability as hidden variables has been given by Allassonni$\backslash$`ere, Amit and Trouv$\backslash$'e (JRSS 2006). Setting the problem in a Bayesian context they proved the consistency of the MAP estimator and provided a simple iterative deterministic algorithm with an EM flavour leading to some reasonable approximations of the MAP estimator under low noise conditions. In this paper we present a stochastic algorithm for approximating the MAP estimator in the spirit of the SAEM algorithm. We prove its convergence to a critical point of the observed likelihood with an illustration on images of handwritten digits.},
archivePrefix = {arXiv},
arxivId = {arXiv:0706.0787v2},
author = {Allassonni{\`{e}}re, St{\'{e}}phanie and Kuhn, Estelle and Trouv{\'{e}}, A},
doi = {10.3150/09-BEJ229},
eprint = {arXiv:0706.0787v2},
file = {:home/alumbreras/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Allassonni{\`{e}}re, Kuhn, Trouv{\'{e}} - 2010 - Construction of Bayesian Deformable Models via Stochastic Approximation Algorithm A Convergence S.pdf:pdf},
issn = {1350-7265},
journal = {Bernouilli},
keywords = {1,60j22,62f10,62f15,62m40,SAEM,ams subject classifications,bayesian,image analysis,in the field of,introduction,map estimation,modeling,modeling of vari-,non rigid-deformable templates,shapes statistics,stochastic approximation algorithms,the statistical analysis and},
mendeley-tags = {SAEM},
number = {3},
pages = {641--678},
title = {{Construction of Bayesian Deformable Models via Stochastic Approximation Algorithm: A Convergence Study}},
url = {http://arxiv.org/abs/0706.0787},
volume = {16},
year = {2010}
}
@article{Acharya2015,
abstract = {Developing models to discover, analyze, and predict clusters within networked entities is an area of active and diverse research. However, many of the existing approaches do not take into consideration pertinent auxiliary information. This paper introduces Joint Gamma Process Poisson Factorization (J-GPPF) to jointly model network and side-information. J-GPPF naturally fits sparse networks, accommodates separately-clustered side information in a principled way, and effectively addresses the computational challenges of analyzing large networks. Evaluated with hold-out link prediction performance on sparse networks (both synthetic and real-world) with side information, J-GPPF is shown to clearly outperform algorithms that only model the network adjacency matrix.},
author = {Acharya, Ayan and Teffer, Dean and Henderson, Jette and Tyler, Marcus and Zhou, Mingyuan},
doi = {10.1007/978-3-319-23528-8_18},
file = {:home/alumbreras/Documentos/Mendeley/Papers/21th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD-2015) - 2015 - Acharya et al. - Gamma Process Poisson Factorizati.pdf:pdf},
isbn = {978-3-319-23528-8; 978-3-319-23527-1},
journal = {21th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD-2015)},
keywords = {gamma process,network modeling,poisson factorization},
pages = {283--299},
title = {{Gamma Process Poisson Factorization for Joint Modeling of Network and Topics}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-23528-8{\_}18},
year = {2015}
}
@article{Dikmen2012,
abstract = {In this paper we describe an alternative to standard nonnegative matrix$\backslash$nfactorization (NMF) for nonnegative dictionary learning, i.e., the task$\backslash$nof learning a dictionary with nonnegative values from nonnegative data,$\backslash$nunder the assumption of nonnegative expansion coefficients. A popular$\backslash$ncost function used for NMF is the Kullback-Leibler divergence, which$\backslash$nunderlies a Poisson observation model. NMF can thus be considered as$\backslash$nmaximization of the joint likelihood of the dictionary and the expansion$\backslash$ncoefficients. This approach lacks optimality because the number of$\backslash$nparameters (which include the expansion coefficients) grows with the$\backslash$nnumber of observations. In this paper we describe variational Bayes and$\backslash$nMonte-Carlo EM algorithms for optimization of the marginal likelihood,$\backslash$ni.e., the likelihood of the dictionary where the expansion coefficients$\backslash$nhave been integrated out (given a Gamma prior). We compare the output of$\backslash$nboth maximum joint likelihood estimation (i.e., standard NMF) and$\backslash$nmaximum marginal likelihood estimation (MMLE) on real and synthetical$\backslash$ndatasets. In particular we present face reconstruction results on CBCL$\backslash$ndataset and text retrieval results over the musiXmatch dataset, a$\backslash$ncollection of word counts in song lyrics. The MMLE approach is shown to$\backslash$nprevent overfitting by automatically pruning out irrelevant dictionary$\backslash$ncolumns, i.e., embedding automatic model order selection.},
author = {Dikmen, Onur and F{\'{e}}votte, C{\'{e}}dric},
doi = {10.1109/TSP.2012.2207117},
file = {:home/alumbreras/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dikmen, F{\'{e}}votte - 2012 - Maximum marginal likelihood estimation for nonnegative dictionary learning in the gamma-poisson model.pdf:pdf},
isbn = {9781457705397},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {Automatic relevance determination,Monte Carlo EM,model order selection,nonnegative matrix factorization,sparse coding,variational EM},
number = {10},
pages = {5163--5175},
title = {{Maximum marginal likelihood estimation for nonnegative dictionary learning in the gamma-poisson model}},
volume = {60},
year = {2012}
}
@article{Paisley2012,
abstract = {Mean-field variational inference is a method for approximate Bayesian posterior inference. It approximates a full posterior distribution with a factorized set of distributions by maximizing a lower bound on the marginal likelihood. This requires the ability to integrate a sum of terms in the log joint likelihood using this factorized distribution. Often not all integrals are in closed form, which is typically handled by using a lower bound. We present an alternative algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound. This method uses control variates to reduce the variance of the stochastic search gradient, in which existing lower bounds can play an important role. We demonstrate the approach on two non-conjugate models: logistic regression and an approximation to the HDP.},
archivePrefix = {arXiv},
arxivId = {1206.6430},
author = {Paisley, John and Blei, David and Jordan, Michael},
eprint = {1206.6430},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Icml - 2012 - Paisley, Blei, Jordan - Variational Bayesian Inference with Stochastic Search.pdf:pdf},
isbn = {978-1-4503-1285-1},
journal = {Icml},
number = {2000},
pages = {1367----1374},
title = {{Variational Bayesian Inference with Stochastic Search}},
url = {http://icml.cc/2012/papers/687.pdf},
year = {2012}
}
@article{Olshausen1996,
author = {Olshausen, Bruno A. and Field, David J.},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Nature - 1996 - Olshausen, Field - Emergence of simple-cell receptive field properties by learning a sparse code for natural images.pdf:pdf},
journal = {Nature},
number = {13},
pages = {607--609},
title = {{Emergence of simple-cell receptive field properties by learning a sparse code for natural images}},
volume = {381},
year = {1996}
}
@inproceedings{DanielD.Lee2001,
address = {Denver, CO, USA},
author = {Lee, Daniel D and Seung, H Sebastian},
booktitle = {Advances in Neural Information Processing Systems (NIPS'2000)},
file = {:home/alumbreras/Documentos/Mendeley/Papers//Advances in Neural Information Processing Systems (NIPS'2000) - 2000 - Lee, Seung - Algorithms for Non-negative Matrix Factorization.pdf:pdf},
keywords = {TM},
mendeley-tags = {TM},
number = {1},
pages = {556--562},
publisher = {MIT Press},
title = {{Algorithms for Non-negative Matrix Factorization}},
volume = {13},
year = {2000}
}
@article{Ghahramani2007,
abstract = {We describe a flexible nonparametric approach to latent variable modelling in which the number of latent variables is unbounded. This approach is based on a probability distribution over equivalence classes of binary matrices with a finite number of rows, corresponding to the data points, and an unbounded number of columns, corresponding to the latent variables. Each data point can be associated with a subset of the possible latent variables, which we re- fer to as the latent features of that data point. The binary variables in the matrix indicate which latent feature is possessed by which data point, and there is a potentially infinite array of features. We derive the distribution over unbounded binary matrices by taking the limit of a distribution over N ×K binary matrices as K → ∞. We define a simple generative processes for this distribution which we call the Indian buffet process (IBP; Griffiths and Ghahramani, 2005, 2006) by analogy to the Chinese restaurant process (Aldous, 1985; Pitman, 2002). The IBP has a single hyperparameter which controls both the number of feature per object and the total number of fea- tures. We describe a two-parameter generalization of the IBP which has addi- tional flexibility, independently controlling the number of features per object and the total number of features in the matrix. The use of this distribution as a prior in an infinite latent feature model is illustrated, and Markov chain Monte Carlo algorithms for inference are described. Keywords},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Ghahramani, Zoubin and Griffiths, Thomas L and Sollich, Peter},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Bayesian Statistics - 2007 - Ghahramani, Griffiths, Sollich - Bayesian Nonparametric Latent Feature Models.pdf:pdf},
isbn = {0199214654},
issn = {1098-6596},
journal = {Bayesian Statistics},
keywords = {and phrases,buffet process,indian,latent variable models,mcmc,non-parametric methods},
pages = {1--25},
pmid = {25246403},
title = {{Bayesian Nonparametric Latent Feature Models}},
url = {http://ai.stanford.edu/{~}tadayuki/papers/miller-phd-dissertation11.pdf},
volume = {8},
year = {2007}
}
@inproceedings{Ranganath2014a,
abstract = {Black box inference allows researchers to easily prototype and evaluate an array of models. Re-cent advances in variational inference allow such algorithms to scale to high dimensions. However, a central question remains: How to specify an ex-pressive variational distribution which maintains efficient computation? To address this, we de-velop hierarchical variational models. In a HIERAR-CHICAL VM, the variational approximation is aug-mented with a prior on its parameters, such that the latent variables are conditionally independent given this shared structure. This preserves the computational efficiency of the original approx-imation, while admitting hierarchically complex distributions for both discrete and continuous la-tent variables. We study HIERARCHICAL VM on a variety of deep discrete latent variable models. HI-ERARCHICAL VM generalizes other expressive varia-tional distributions and maintains higher fidelity to the posterior.},
archivePrefix = {arXiv},
arxivId = {1511.02386},
author = {Ranganath, Rajesh and Tran, Dustin and Blei, David M},
booktitle = {Proceedings of the 33rd International Conference on Machine Learning},
eprint = {1511.02386},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Proceedings of the 33rd International Conference on Machine Learning - 2016 - Ranganath, Tran, Blei - Hierarchical Variational Models.pdf:pdf},
isbn = {1511.02386},
pages = {1--9},
title = {{Hierarchical Variational Models}},
year = {2016}
}
@article{Sun2014,
author = {Sun, Dl and Fevotte, Cedric},
doi = {10.1109/ICASSP.2014.6854796},
file = {:home/alumbreras/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun, Fevotte - 2014 - Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization With the Beta-Divergence.pdf:pdf},
isbn = {9781479928934},
journal = {Icassp},
pages = {6242--6246},
title = {{Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization With the Beta-Divergence}},
url = {http://statweb.stanford.edu/{~}dlsun/papers/nmf{\_}admm.pdf},
year = {2014}
}
@inproceedings{Bishop1999,
author = {Bishop, Christopher M},
booktitle = {Advances in Neural Information Processing Systems},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Advances in Neural Information Processing Systems - 1999 - Bishop - Bayesian PCA.pdf:pdf},
title = {{Bayesian PCA}},
year = {1999}
}
@article{Huang2014,
abstract = {We develop a Bayesian nonparametric model for reconstructing magnetic resonance images (MRI) from highly undersampled k-space data. We perform dictionary learning as part of the image reconstruction process. To this end, we use the beta process as a nonparametric dictionary learning prior for representing an image patch as a sparse combination of dictionary elements. The size of the dictionary and the patch-specific sparsity pattern are inferred from the data, in addition to other dictionary learning variables. Dictionary learning is performed directly on the compressed image, and so is tailored to the MRI being considered. In addition, we investigate a total variation penalty term in combination with the dictionary learning model, and show how the denoising property of dictionary learning removes dependence on regularization parameters in the noisy setting. We derive a stochastic optimization algorithm based on Markov Chain Monte Carlo (MCMC) for the Bayesian model, and use the alternating direction method of multipliers (ADMM) for efficiently performing total variation minimization. We present empirical results on several MRI, which show that the proposed regularization framework can improve reconstruction accuracy over other methods.},
archivePrefix = {arXiv},
arxivId = {1302.2712},
author = {Huang, Yue and Paisley, John and Lin, Qin and Ding, Xinghao and Fu, Xueyang and Zhang, Xiao Ping},
doi = {10.1109/TIP.2014.2360122},
eprint = {1302.2712},
file = {:home/alumbreras/Documentos/Mendeley/Papers/IEEE Transactions on Image Processing - 2014 - Huang et al. - Bayesian nonparametric dictionary learning for compressed sensing MRI.pdf:pdf},
isbn = {2011121051},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Bayesian nonparametrics,compressed sensing,dictionary learning,magnetic resonance imaging},
number = {12},
pages = {5007--5019},
pmid = {25265609},
title = {{Bayesian nonparametric dictionary learning for compressed sensing MRI}},
volume = {23},
year = {2014}
}
@article{Lewicki2000,
author = {Lewicki, Michael S and Sejnowski, Terrence J},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Neural Computation - 2000 - Lewicki, Sejnowski - Learning Overcomplete Representations.pdf:pdf},
journal = {Neural Computation},
pages = {337--365},
title = {{Learning Overcomplete Representations}},
volume = {12},
year = {2000}
}
@article{Gopalan2014,
abstract = {We develop a Bayesian nonparametric Poisson factorization model for recommendation systems. Poisson factorization implicitly models each user's limited budget of attention (or money) that allows consumption of only a small subset of the available items. In our Bayesian nonparametric variant, the number of latent components is theoretically unbounded and effectively estimated when computing a posterior with observed user behavior data. To approximate the posterior, we develop an efficient variational inference algorithm. It adapts the dimensionality of the latent components to the data, only requires iteration over the user/item pairs that have been rated, and has computational complexity on the same order as for a parametric model with fixed dimensionality. We studied our model and algorithm with large realworld data sets of user-movie preferences. Our model eases the computational burden of searching for the number of latent components and gives better predictive performance than its parametric counterpart.},
author = {Gopalan, Prem and Ruiz, Francisco J. R. and Ranganath, Rajesh and Blei, David M.},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Journal of Machine Learning Research - 2014 - Gopalan et al. - Bayesian Nonparametric Poisson Factorization for Recommendation Systems.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
pages = {275--283},
title = {{Bayesian Nonparametric Poisson Factorization for Recommendation Systems}},
url = {http://jmlr.org/proceedings/papers/v33/gopalan14.pdf},
volume = {33},
year = {2014}
}
@article{Paisley2010,
abstract = {We present a Bayesian model for image interpolation and dictionary learning that uses two nonparametric priors for sparse signal representations: the beta process and the Dirichlet process. Additionally, the model uses spatial information within the image to encourage sharing of information within image subregions. We derive a hybrid MAP/Gibbs sampler, which performs Gibbs sampling for the latent indicator variables and MAP estimation for all other parameters. We present experimental results, where we show an improvement over other state-of-the-art algorithms in the low-measurement regime.},
author = {Paisley, John and Zhou, Mingyuan and Sapiro, Guillermo and Carin, Lawrence},
doi = {10.1109/ICIP.2010.5653350},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Proceedings - International Conference on Image Processing, ICIP - 2010 - Paisley et al. - Nonparametric image interpolation and diction.pdf:pdf},
isbn = {9781424479948},
issn = {15224880},
journal = {Proceedings - International Conference on Image Processing, ICIP},
keywords = {Bayesian models,Beta process,Dictionary learning,Dirichlet process,Image interpolation},
pages = {1869--1872},
title = {{Nonparametric image interpolation and dictionary learning using spatially-dependent dirichlet and beta process priors}},
year = {2010}
}
@inproceedings{Srebro2003,
abstract = {We study the common problem of approximating a target matrix with a matrix of lower rank. We provide a simple and efficient (EM) algorithm for solving weighted low-rank approximation problems, which, unlike their unweighted version, do not admit a closedform solution in general. We analyze, in addition, the nature of locally optimal solutions that arise in this context, demonstrate the utility of accommodating the weights in reconstructing the underlying low-rank representation, and extend the formulation to non-Gaussian noise models such as logistic models. Finally, we apply the methods developed to a collaborative filtering task.},
author = {Srebro, Nathan and Jaakkola, Tommi},
booktitle = {20th International Conference on Machine Learning},
doi = {10.1.1.5.3301},
file = {:home/alumbreras/Documentos/Mendeley/Papers/20th International Conference on Machine Learning - 2003 - Srebro, Jaakkola - Weighted low-rank approximations.pdf:pdf},
isbn = {1-57735-189-4},
pages = {720--727},
title = {{Weighted low-rank approximations}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.5.3301},
volume = {3},
year = {2003}
}
@inproceedings{Collins2001,
author = {Collins, Michael and Dasgupta, Sanjoy and Schapire, Robert E and Park, Florham},
booktitle = {Advances in Neural Information Processing Systems},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Advances in Neural Information Processing Systems - 2001 - Collins et al. - A Generalization of Principal Component Analysis to the Expo.pdf:pdf},
title = {{A Generalization of Principal Component Analysis to the Exponential Family}},
year = {2001}
}
@article{Sudderth2006,
annote = {En el 2.5 hqce unq introduccion a procesos de Dirichlet. Pero asume algunos conocimentos previos.},
author = {Sudderth, Erik B},
file = {:home/alumbreras/Documentos/Mendeley/Papers/PhD Thesis - 2006 - Sudderth - Graphical Models for Visual Object Recognition and Tracking.pdf:pdf},
journal = {PhD Thesis},
title = {{Graphical Models for Visual Object Recognition and Tracking}},
year = {2006}
}
@article{Dhillon2005,
abstract = {Nonnegative matrix approximation (NNMA) is a recent technique for di-mensionality reduction and data analysis that yields a parts based, sparse nonnegative representation for nonnegative input data. NNMA has found a wide variety of applications, including text analysis, document cluster-ing, face/image recognition, language modeling, speech processing and many others. Despite these numerous applications, the algorithmic de-velopment for computing the NNMA factors has been relatively defi-cient. This paper makes algorithmic progress by modeling and solving (using multiplicative updates) new generalized NNMA problems that minimize Bregman divergences between the input matrix and its low-rank approximation. The multiplicative update formulae in the pioneer-ing work by Lee and Seung [11] arise as a special case of our algorithms. In addition, the paper shows how to use penalty functions for incorporat-ing constraints other than nonnegativity into the problem. Further, some interesting extensions to the use of " link " functions for modeling non-linear relationships are also discussed.},
author = {Dhillon, Inderjit S and Sra, Suvrit},
doi = {10.1.1.72.5975},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Advances in neural information processing systems - 2005 - Dhillon, Sra - Generalized Nonnegative Matrix Approximations with Bregman Div.pdf:pdf},
isbn = {9780262232531},
issn = {1049-5258},
journal = {Advances in neural information processing systems},
pages = {283--290},
title = {{Generalized Nonnegative Matrix Approximations with Bregman Divergences}},
volume = {19},
year = {2005}
}
@article{Knowles2011,
abstract = {A nonparametric Bayesian extension of Factor Analysis (FA) is proposed where observed data Y is modeled as a linear superposition, G, of a potentially infinite number of hidden factors, X. The Indian Buffet Process (IBP) is used as a prior on G to incorporate sparsity and to allow the number of latent features to be inferred. The model's utility for modeling gene expression data is investigated using randomly generated datasets based on a known sparse connectivity matrix for E. Coli, and on three biological datasets of increasing complexity.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.6293v2},
author = {Knowles, David a. and Ghahramani, Zoubin},
doi = {10.1214/10-AOAS435},
eprint = {arXiv:1011.6293v2},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Unknown - 2011 - Carlo - NONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS WITH APPLICATION TO GENE EXPRESSION MODELING By David Knowles 1 and.pdf:pdf},
isbn = {1932-6157},
issn = {1941-7330},
journal = {The Annals of Applied Statistics},
keywords = {and phrases,factor analysis,markov chain,nonparametric bayes,sparsity},
number = {2B},
pages = {1534--1552},
title = {{Nonparametric Bayesian sparse factor models with application to gene expression modeling}},
url = {https://code.google.com/p/nsfa/},
volume = {5},
year = {2011}
}
@article{Wang2013b,
abstract = {Mean-field variational methods are widely used for approximate posterior inference in many prob-abilistic models. In a typical application, mean-field methods approximately compute the posterior with a coordinate-ascent optimization algorithm. When the model is conditionally conjugate, the coordinate updates are easily derived and in closed form. However, many models of interest—like the correlated topic model and Bayesian logistic regression—are nonconjugate. In these models, mean-field methods cannot be directly applied and practitioners have had to develop variational algorithms on a case-by-case basis. In this paper, we develop two generic methods for nonconju-gate models, Laplace variational inference and delta method variational inference. Our methods have several advantages: they allow for easily derived variational algorithms with a wide class of nonconjugate models; they extend and unify some of the existing algorithms that have been derived for specific models; and they work well on real-world data sets. We studied our methods on the correlated topic model, Bayesian logistic regression, and hierarchical Bayesian logistic regression.},
archivePrefix = {arXiv},
arxivId = {1209.4360},
author = {Wang, Chong and Blei, David M},
eprint = {1209.4360},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Journal of Machine Learning Research - 2013 - Wang, Blei - Variational Inference in Nonconjugate Models.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {Laplace approximations,nonconjugate models,the multivariate delta method,variational inference},
pages = {1005--1031},
title = {{Variational Inference in Nonconjugate Models}},
volume = {14},
year = {2013}
}
@article{Tipping1997,
author = {Tipping, Michael E. and Bishop, Christopher M.},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Journal of the Royal Statistical Society Series - 1997 - Tipping, Bishop - Probabilistic Principal Component Analysis.pdf:pdf},
journal = {Journal of the Royal Statistical Society Series},
pages = {611--622},
title = {{Probabilistic Principal Component Analysis}},
volume = {61},
year = {1997}
}
@article{Burda2015,
abstract = {The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
archivePrefix = {arXiv},
arxivId = {1509.00519},
author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
eprint = {1509.00519},
file = {:home/alumbreras/Documentos/Mendeley/Papers/ICLR - 2015 - Burda, Grosse, Salakhutdinov - Importance Weighted Autoencoders.pdf:pdf},
isbn = {1509.00519},
issn = {1312.6114v10},
journal = {ICLR},
pages = {1--12},
title = {{Importance Weighted Autoencoders}},
url = {http://arxiv.org/abs/1509.00519},
year = {2015}
}
@article{Srebro2005,
abstract = {We present a novel approach to collaborative prediction, using low-norm instead of low-rank factorizations. The approach is inspired by, and has strong connections to, large-margin linear discrimination. We show how to learn low-norm factorizations by solving a semi-definite program, and discuss generalization error bounds for them.},
author = {Srebro, Nathan and Rennie, Jason D M and Jaakkola, Tommi S},
doi = {10.1.1.59.118},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Advances in Neural Information Processing Systems - 2005 - Srebro, Rennie, Jaakkola - Maximum-Margin Matrix Factorization.pdf:pdf},
isbn = {0262195348},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1329--1336},
title = {{Maximum-Margin Matrix Factorization}},
volume = {17},
year = {2005}
}
@article{Li2012,
abstract = {Non-negative matrix factorization (NMF) provides a lower rank approximation of a matrix. Due to nonnegativity im- posed on the factors, it gives a latent structure that is often more physically meaningful than other lower rank approxi- mations such as singular value decomposition (SVD). Most of the algorithms proposed in literature for NMF have been based on minimizing the Frobenius norm. This is partly due to the fact that the minimization problem based on the Frobenius norm provides much more flexibility in alge- braic manipulation than other divergences. In this paper we propose a fast NMF algorithm that is applicable to general Bregman divergences. Through Taylor series expansion of the Bregman divergences, we reveal a relationship between Bregman divergences and Euclidean distance. This key rela- tionship provides a new direction for NMF algorithms with general Bregman divergences when combined with the scalar block coordinate descent method. The proposed algorithm generalizes several recently proposed methods for computa- tion of NMF with Bregman divergences and is computation- ally faster than existing alternatives. We demonstrate the effectiveness of our approach with experiments conducted on artificial as well as real world data.},
author = {Li, Liangda and Lebanon, Guy and Park, Haesun},
doi = {10.1145/2339530.2339582},
file = {:home/alumbreras/Documentos/Mendeley/Papers/ACM SIGKDD international conference on Knowledge discovery and data mining - 2012 - Li, Lebanon, Park - Fast bregman divergence NMF usin.pdf:pdf},
isbn = {9781450314626},
journal = {ACM SIGKDD international conference on Knowledge discovery and data mining},
keywords = {bregman divergences,euclidean distance,non-negative matrix factorization,taylor series expansion},
pages = {307},
title = {{Fast bregman divergence NMF using taylor expansion and coordinate descent}},
url = {http://dl.acm.org/citation.cfm?doid=2339530.2339582},
year = {2012}
}
@incollection{Paisley2014,
author = {Paisley, John and Blei, David M. and Jordan, Michael I.},
booktitle = {Handbook of Mixed Membership Models},
pages = {203--222},
publisher = {Chapman and Hall/CRC},
title = {{Bayesian Nonnegative Matrix Factorization with Stochastic Variational Inference}},
year = {2014}
}
@article{Lawrence2005,
abstract = {Summarising a high dimensional data-set with a low dimensional embedding is a standard approach for exploring its structure. In this paper we provide an overview of some existing techniques for discovering such embeddings. We then introduce a novel probabilistic interpretation of principal component analysis (PCA) that we term dual probabilistic PCA (DPPCA). The DPPCA model has the additional advantage that the linear mappings from the embedded space can easily be non-linearised through Gaussian processes. We refer to this model as a Gaussian process latent variable model (GPLVM). We develop a practical algorithm for GPLVMs which allow for non-linear mappings from the embedded space giving a non-linear probabilistic version of PCA. We develop the new algorithm to provide a principled approach to handling discrete valued data and missing attributes. We demonstrate the algorithm on a range of real-world and artificially generated data-sets and finally, through analysis of the GPLVM objective function, we relate the algorithm to popular spectral techniques such as kernel PCA and multidimensional scaling.},
author = {Lawrence, Neil},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Journal ofMachine Learning Research - 2005 - Lawrence - Probabilistic non-linear Principal Component Analysis with Gaussian Process Late.pdf:pdf},
isbn = {1532-4435},
issn = {1476-4687},
journal = {Journal ofMachine Learning Research},
keywords = {Learning/Statistics {\&} Optimisation,Theory {\&} Algorithms},
pages = {1783--1816},
title = {{Probabilistic non-linear Principal Component Analysis with Gaussian Process Latent Variable Models}},
url = {http://eprints.pascal-network.org/archive/00000914/},
volume = {6},
year = {2005}
}
@article{Yang2011a,
author = {Yang, Zhirong and Zhang, He and Yuan, Zhijian and Oja, Erkki},
doi = {10.1007/978-3-642-21735-7_31},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Artificial Neural Networks and Machine Learning -- ICANN 2011 21st International Conference on Artificial Neural Networks, Espoo, Finlan.pdf:pdf},
isbn = {978-3-642-21735-7},
journal = {Artificial Neural Networks and Machine Learning -- ICANN 2011: 21st International Conference on Artificial Neural Networks, Espoo, Finland, June 14-17, 2011, Proceedings, Part I},
pages = {250--257},
title = {{Kullback-Leibler Divergence for Nonnegative Matrix Factorization}},
url = {http://dx.doi.org/10.1007/978-3-642-21735-7{\_}31},
year = {2011}
}
@article{Titsias,
abstract = {We present a probability distribution over non-negative integer valued matrices with possibly an infinite number of columns. We also derive a stochastic process that reproduces this distribution over equivalence classes. This model can play the role of the prior in nonparametric Bayesian learning scenarios where multiple latent features are associated with the observed data and each feature can have multiple appearances or occurrences within each data point. Such data arise naturally when learning visual object recognition systems from unlabelled images. Together with the nonparametric prior we consider a likelihood model that explains the visual appearance and location of local image patches. Inference with this model is carried out using a Markov chain Monte Carlo algorithm.},
author = {Titsias, Michalis K},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Advances in Neural Information Processing Systems (NIPS) 20 - 2008 - Titsias - The Infinite Gamma-Poisson Feature Model.pdf:pdf},
isbn = {160560352X},
journal = {Advances in Neural Information Processing Systems (NIPS) 20},
keywords = {GaP,Gamma-Poisson,non-negative integer valued matrices,nonparametric Bayes,unlabelled images},
title = {{The Infinite Gamma-Poisson Feature Model}},
year = {2008}
}
@article{Hoffman2010a,
abstract = {Recent research in machine learning has focused on breaking audio spectrograms into separate sources of sound using latent variable decompositions. These methods require that the number of sources be specified in advance, which is not always possible. To address this problem, we develop Gamma Process Nonnegative Matrix Factorization (GaP-NMF), a Bayesian nonpara- metric approach to decomposing spectrograms. The assumptions behind GaP-NMF are based on research in signal processing regarding the expected distributions of spectrogram data, and GaP-NMF automatically discovers the number of latent sources. We derive a mean-field variational inference algorithm and evaluate GaP-NMF on both synthetic data and recorded music.},
author = {Hoffman, Matthew D and Blei, David M and Cook, Perry R},
doi = {10.1.1.168.2855},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Proceedings of the 27th International Conference on Machine Learning (ICML-10) - 2010 - Hoffman, Blei, Cook - Bayesian Nonparametric Mat.pdf:pdf},
isbn = {9781605589077},
journal = {Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
keywords = {BNP,NMF},
mendeley-tags = {BNP,NMF},
pages = {439--446},
title = {{Bayesian Nonparametric Matrix Factorization for Recorded Music}},
url = {http://www.icml2010.org/papers/523.pdf},
year = {2010}
}
@article{Wainwright2007,
author = {Wainwright, Martin J. and Jordan, Michael I.},
doi = {10.1561/2200000001},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Foundations and Trends{\textregistered} in Machine Learning - 2007 - Wainwright, Jordan - Graphical Models, Exponential Families, and Variational Infer.pdf:pdf},
issn = {1935-8237},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
number = {1–2},
pages = {1--305},
title = {{Graphical Models, Exponential Families, and Variational Inference}},
url = {http://www.nowpublishers.com/article/Details/MAL-001},
volume = {1},
year = {2007}
}
@article{Ranganath2013,
abstract = {Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm gen-erally requires significant model-specific anal-ysis. These efforts can hinder and deter us from quickly developing and exploring a vari-ety of models for a problem at hand. In this paper, we present a " black box " variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochas-tic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational dis-tribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations. We evaluate our method against the correspond-ing black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling meth-ods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.},
archivePrefix = {arXiv},
arxivId = {1401.0118},
author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David M},
eprint = {1401.0118},
file = {:home/alumbreras/Documentos/Mendeley/Papers/AISTATS - 2013 - Ranganath, Gerrish, Blei - Black Box Variational Inference.pdf:pdf},
isbn = {1401.0118},
issn = {15337928},
journal = {AISTATS},
title = {{Black Box Variational Inference}},
url = {http://www.jmlr.org/proceedings/papers/v33/ranganath14.pdf$\backslash$nhttp://www.cs.columbia.edu/{~}blei/papers/RanganathGerrishBlei2014.pdf},
volume = {33},
year = {2013}
}
@article{Schmidt2009,
abstract = {We present a Bayesian treatment of non-negative matrix factorization (NMF), based on a normal likelihood and exponential priors, and derive an efficient Gibbs sampler to approximate the posterior density of the NMF factors. On a chemical brain imaging data set, we show that this improves interpretability by providing uncertainty estimates. We discuss how the Gibbs sampler can be used for model order selection by estimating the marginal likelihood, and compare with the Bayesian information criterion. For computing the maximum a posteriori estimate we present an iterated conditional modes algorithm that rivals existing state-of-the-art NMF algorithms on an image feature extraction problem.},
author = {Schmidt, Mikkel N. and Winther, Ole and Hansen, Lars K.},
doi = {10.1007/978-3-642-00599-2_68},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Independent Component Analysis and Signal Separation - 2009 - Schmidt, Winther, Hansen - Bayesian Non-negative Matrix Factorization.pdf:pdf},
isbn = {978-3-642-00598-5},
issn = {0302-9743},
journal = {Independent Component Analysis and Signal Separation},
pages = {540--547},
title = {{Bayesian Non-negative Matrix Factorization}},
volume = {5441},
year = {2009}
}
@article{Xu2012a,
abstract = {We present a probabilistic formulation of max-margin matrix factorization and build accordingly a nonparametric Bayesian model which automatically resolves the unknown number of latent factors. Our work demonstrates a successful example that integrates Bayesian nonparametrics and max-margin learning, which are conventionally two separate paradigms and enjoy complementary advantages. We develop an efficient variational algorithm for posterior inference, and our extensive empirical studies on large-scale MovieLens and EachMovie data sets appear to justify the aforementioned dual advantages.},
author = {Xu, Minjie and Zhu, Jun and Zhang, Bo},
doi = {10.1109/ICDM.2014.43},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Nips 2012 - 2012 - Xu, Zhu, Zhang - Nonparametric Max-Margin Matrix Factorization for Collaborative Prediction.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Nips 2012},
pages = {1--9},
title = {{Nonparametric Max-Margin Matrix Factorization for Collaborative Prediction.}},
url = {https://papers.nips.cc/paper/4581-nonparametric-max-margin-matrix-factorization-for-collaborative-prediction.pdf},
year = {2012}
}
@article{Salimans2014,
archivePrefix = {arXiv},
arxivId = {1410.6460},
author = {Salimans, Tim},
eprint = {1410.6460},
file = {:home/alumbreras/Documentos/Mendeley/Papers/arXiv preprint arXiv1410.6460 - 2014 - Salimans - Markov Chain Monte Carlo and Variational Inference Bridging the Gap.pdf:pdf},
isbn = {1410.6460},
journal = {arXiv preprint arXiv:1410.6460},
number = {2},
pages = {1--8},
title = {{Markov Chain Monte Carlo and Variational Inference: Bridging the Gap}},
url = {http://arxiv.org/abs/1410.6460},
year = {2014}
}
@article{Dikmen2011b,
author = {Dikmen, Onur and F{\'{e}}votte, C{\'{e}}dric},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Icassp - 2011 - Dikmen, F{\'{e}}votte - Maximum Marginal Likelihood Estimation for Nonnegative Dictionary Learning.pdf:pdf},
isbn = {9781457705397},
journal = {Icassp},
pages = {1992--1995},
title = {{Maximum Marginal Likelihood Estimation for Nonnegative Dictionary Learning}},
year = {2011}
}
@article{Salakhutdinov2008,
abstract = {Low-rank matrix approximation methods provide one of the simplest and most effective approaches to collaborative filtering. Such models are usually fitted to data by finding a MAP estimate of the model parameters, a procedure that can be performed efficiently even on very large datasets. However, unless the regularization parameters are tuned carefully, this approach is prone to overfitting because it finds a single point estimate of the parameters. In this paper we present a fully Bayesian treatment of the Probabilistic Matrix Factorization (PMF) model in which model capacity is controlled automatically by integrating over all model parameters and hyperparameters. We show that Bayesian PMF models can be efficiently trained using Markov chain Monte Carlo methods by applying them to the Netflix dataset, which consists of over 100 million movie ratings. The resulting models achieve significantly higher prediction accuracy than PMF models trained using MAP estimation.},
author = {Salakhutdinov, R and Mnih, A},
doi = {10.1145/1390156.1390267},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Proceedings of the 25th International Conference on Machine Learning (2008) - 2008 - Salakhutdinov, Mnih - Bayesian probabilistic matrix.pdf:pdf},
isbn = {9781605582054},
issn = {1049-5258},
journal = {Proceedings of the 25th International Conference on Machine Learning (2008)},
number = {2005},
pages = {880--887},
title = {{Bayesian probabilistic matrix factorization using Markov chain Monte Carlo.}},
url = {http://discovery.ucl.ac.uk/63251/},
volume = {25},
year = {2008}
}
@article{Alquier2015,
abstract = {The PAC-Bayesian approach is a powerful set of techniques to derive non- asymptotic risk bounds for random estimators. The corresponding optimal distribution of estimators, usually called the Gibbs posterior, is unfortunately intractable. One may sample from it using Markov chain Monte Carlo, but this is often too slow for big datasets. We consider instead variational approximations of the Gibbs posterior, which are fast to compute. We undertake a general study of the properties of such approximations. Our main finding is that such a variational approximation has often the same rate of convergence as the original PAC-Bayesian procedure it approximates. We specialise our results to several learning tasks (classification, ranking, matrix completion),discuss how to implement a variational approximation in each case, and illustrate the good properties of said approximation on real datasets.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.04091v2},
author = {Alquier, Pierre and Ridgway, James and Chopin, Nicolas},
eprint = {arXiv:1506.04091v2},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Unknown - 2015 - Alquier, Ridgway, Chopin - On the properties of variational approximations of Gibbs posteriors.pdf:pdf},
pages = {1--38},
title = {{On the properties of variational approximations of Gibbs posteriors}},
year = {2015}
}
@article{Knowles2007,
abstract = {A nonparametric Bayesian extension of Independent Components Analysis (ICA) is proposed where observed data Y is modelled as a linear superposition, G, of a potentially infinite number of hidden sources, X. Whether a given source is active for a specific data point is specified by an infinite binary matrix, Z. The resulting sparse representation allows increased data reduction compared to standard ICA. We define a prior on Z using the Indian Buffet Process (IBP). We describe four variants of the model, with Gaussian or Laplacian priors on X and the one or two-parameter IBPs. We demonstrate Bayesian inference under these models using a Markov Chain Monte Carlo (MCMC) algorithm on synthetic and gene expression data and compare to standard ICA algorithms.},
author = {Knowles, David and Ghahramani, Zoubin},
doi = {10.1007/978-3-540-74494-8_48},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Proceedings of the 7th international conference on Independent component analysis and signal separation - 2007 - Knowles, Ghahramani - I.pdf:pdf},
isbn = {978-3-540-74493-1},
issn = {03029743},
journal = {Proceedings of the 7th international conference on Independent component analysis and signal separation},
number = {1},
pages = {381--388},
title = {{Infinite Sparse Factor Analysis and Infinite Independent Components Analysis}},
year = {2007}
}
@article{Delyon1999,
abstract = {The expectation-maximization (EM) algorithm is a powerful computational technique for locating maxima of functions. It is widely used in statistics for maximum likelihood or maximum a posteriori estimation in incomplete data models. In certain situations, however, this method is not applicable because the expectation step cannot be performed in closed form. To deal with these problems, a novel method is introduced, the stochastic approximation EM (SAEM), which replaces the expectation step of the EM algorithm by one iteration of a stochastic approximation procedure. The convergence of the SAEM algorithm is established under conditions that are applicable to many practical situations. Moreover, it is proved that, under mild additional conditions, the attractive stationary points of the SAEM algorithm correspond to the local maxima of the function. presented to support our findings. CR - Copyright {\&}{\#}169; 1999 Institute of Mathematical Statistics},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Delyon, Bernard and Lavielle, Marc and Moulines, Eric},
doi = {10.2307/120120},
eprint = {arXiv:1011.1669v3},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Annals of Statistics - 1999 - Delyon, Lavielle, Moulines - Convergence of a stochastic approximation version of the EM algorithm.pdf:pdf},
isbn = {9788578110796},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Em algorithm,Incomplete data,Maximum likelihood,Missing data,Monte Carlo algorithm,Optimization,SAEM,Simulation,Stochastic algorithm},
mendeley-tags = {SAEM},
number = {1},
pages = {94--128},
pmid = {92},
title = {{Convergence of a stochastic approximation version of the EM algorithm}},
volume = {27},
year = {1999}
}
@article{Nakajima2013,
author = {Nakajima, Shinichi and Sugiyama, Masashi and Babacan, S Derin and Tomioka, Ryota},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Journal of Machine Learning Research - 2013 - Nakajima et al. - Global Analytic Solution of Fully-observed Variational Bayesian Matrix F.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {1--37},
title = {{Global Analytic Solution of Fully-observed Variational Bayesian Matrix Factorization}},
volume = {14},
year = {2013}
}
@article{Titsias2014,
abstract = {We propose a simple and effective variational inference algorithm based on stochastic optimi-sation that can be widely applied for Bayesian non-conjugate inference in continuous parameter spaces. This algorithm is based on stochastic ap-proximation and allows for efficient use of gra-dient information from the model joint density. We demonstrate these properties using illustra-tive examples as well as in challenging and di-verse Bayesian inference problems such as vari-able selection in logistic regression and fully Bayesian inference over kernel hyperparameters in Gaussian process regression.},
author = {Titsias, Michalis and L{\'{a}}zaro-Gredilla, Miguel},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Proceedings of The 31st International Conference on Machine Learning - 2014 - Titsias, L{\'{a}}zaro-Gredilla - Doubly Stochastic Variational.pdf:pdf},
isbn = {9781634393973},
journal = {Proceedings of The 31st International Conference on Machine Learning},
pages = {1971--1979},
title = {{Doubly Stochastic Variational Bayes for non-Conjugate Inference}},
url = {http://jmlr.org/proceedings/papers/v32/titsias14},
volume = {32},
year = {2014}
}
@article{Anderson1952,
author = {Anderson, T. W. and Darling, D. A.},
doi = {10.1214/09-STS284},
file = {:home/alumbreras/Documentos/Mendeley/Papers/The Annals of Mathematical Statistics - 1952 - Anderson, Darling - Integrated Likelihood Methods for Eliminating Nuisance Parameters com.pdf:pdf},
isbn = {0940600315},
journal = {The Annals of Mathematical Statistics},
number = {2},
pages = {193--212},
title = {{Integrated Likelihood Methods for Eliminating Nuisance Parameters: comment}},
volume = {23},
year = {1952}
}
@article{Hoffman2015,
abstract = {Stochastic variational inference makes it possible to approximate posterior distributions induced by large datasets quickly using stochastic optimization. The algorithm relies on the use of fully factorized variational distributions. However, this “mean-field” independence approximation limits the fidelity of the posterior approximation, and introduces local optima. We show how to relax the mean-field approximation to allow arbitrary dependencies between global parameters and local hidden variables, producing better parameter estimates by reducing bias, sensitivity to local optima, and sensitivity to hyperparameters.},
archivePrefix = {arXiv},
arxivId = {1404.4114},
author = {Hoffman, Matthew D and Blei, David M},
doi = {citeulike-article-id:10852147},
eprint = {1404.4114},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics - 2015 - Hoffman, Blei - Structured Sto.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics},
pages = {361--369},
title = {{Structured Stochastic Variational Inference}},
volume = {38},
year = {2015}
}
@article{Lefevre2011,
author = {Lef{\`{e}}vre, A and Bach, Francis R and F{\'{e}}votte, C},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Proc. 23{\{}e{\}} colloque GRETSI sur le Traitement du Signal et des Images - 2011 - Lef{\`{e}}vre, Bach, F{\'{e}}votte - Factorisation de matrices str.pdf:pdf},
journal = {Proc. 23{\{}e{\}} colloque GRETSI sur le Traitement du Signal et des Images},
title = {{Factorisation de matrices structur{\'{e}}e en groupes avec la divergence d'Itakura-Saito}},
url = {http://perso.telecom-paristech.fr/{~}fevotte/Proceedings/gretsi11c.pdf},
year = {2011}
}
@article{Dikmen2011a,
author = {Dikmen, Onur and F{\'{e}}votte, C{\'{e}}dric},
file = {:home/alumbreras/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dikmen, F{\'{e}}votte - 2011 - Nonnegative dictionary learning in the exponential noise model for adaptive music signal representation.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {1--9},
title = {{Nonnegative dictionary learning in the exponential noise model for adaptive music signal representation}},
year = {2011}
}
@article{HintonGeoffreyE.1994,
abstract = {An autoencoder network uses a aset of recognition weights to convert the input veecotre into a code vectore. It then uses set of generative weights to convert the code vector inot an approximate reconstruction of the input vector. We derive and objective function for training autoencoderss based on the minimum descrption length (MDL) principle. The aim is to minimize the information required to describe both the code vector and the reconsrtuction error. WE show that this information is minimzed by choosing code vectors stochastiacally according to a Boltzmann distribution, where the generative weights define the energy of each possible code vector given the input vector. Unfortunately, if the code vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible code vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this approixmation gives an upper bound on the description length. Even hen this bound is poor, it can be used a Lyapuov function for learning both the generative and recognition weights. We demonstrate that this approach can be used to learn factorial codes.},
author = {{Hinton, Geoffrey E.}, and Richard S. Zemel.},
doi = {10.1021/jp906511z},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Advances in Neural Information Processing Systems - 1994 - Hinton, Geoffrey E. - Autoencoders, Minimum Description Length and Helmholtz.pdf:pdf},
isbn = {1049-5258},
issn = {15205207},
journal = {Advances in Neural Information Processing Systems},
number = {3},
pages = {--},
pmid = {20148535},
title = {{Autoencoders, Minimum Description Length and Helmholtz free Energy}},
url = {https://www.cs.toronto.edu/{~}hinton/absps/cvq.pdf},
volume = {3},
year = {1994}
}
@article{Brouwer,
author = {Brouwer, Thomas A},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Unknown - Unknown - Brouwer - Probabilistic non-negative matrix factorisation and extensions.pdf:pdf},
pages = {1--35},
title = {{Probabilistic non-negative matrix factorisation and extensions}}
}
@article{Welling2011,
abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an in-built protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a ``sampling threshold'' and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
archivePrefix = {arXiv},
arxivId = {arXiv:1203.5753v5},
author = {Welling, M. and Teh, Y.-W.},
eprint = {arXiv:1203.5753v5},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Proceedings of the 28th International Conference on Machine Learning - 2011 - Welling, Teh - Bayesian Learning via Stochastic Gradient L.pdf:pdf},
isbn = {978-1-4503-0619-5},
journal = {Proceedings of the 28th International Conference on Machine Learning},
keywords = {Bayesian learning,ICML,machine learning,online learning},
pages = {681--688},
title = {{Bayesian Learning via Stochastic Gradient Langevin Dynamics}},
year = {2011}
}
@article{Teh2007,
abstract = {The Indian buffet process (IBP) is a Bayesian nonparametric distribution whereby objects are modelled using an unbounded number of latent features. In this paper we derive a stick-breaking representation for the IBP. Based on this new rep- resentation, we develop slice samplers for the IBP that are efficient, easy to implement and are more generally applicable than the currently available Gibbs sampler. This representation, along with the work of Thibaux and Jordan (17), also illuminates interesting theoretical connec- tions between the IBP, Chinese restaurant pro- cesses, Beta processes and Dirichlet processes.},
author = {Teh, Yee Whye and Dilan, G{\"{o}}r{\"{u}}r and Ghahramani, Zoubin},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics - Unknown - Teh et al. - Stick-breaking C.pdf:pdf},
issn = {15324435},
journal = {Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics},
pages = {556--563},
title = {{Stick-breaking Construction for the Indian Buffet Process}},
volume = {2},
year = {2007}
}
@article{Maclaurin2015,
abstract = {Markov chain Monte Carlo (MCMC) is a popular and successful general-purpose tool for Bayesian inference. However, MCMC cannot be practically applied to large data sets because of the prohibitive cost of evaluating every likelihood term at every iteration. Here we present Firefly Monte Carlo (FlyMC) an auxiliary variable MCMC algorithm that only queries the likelihoods of a potentially small subset of the data at each iteration yet simulates from the exact posterior distribution, in contrast to recent proposals that are approximate even in the asymptotic limit. FlyMC is compatible with a wide variety of modern MCMC algorithms, and only requires a lower bound on the per-datum likelihood factors. In experiments, we find that FlyMC generates samples from the posterior more than an order of magnitude faster than regular MCMC, opening up MCMC methods to larger datasets than were previously considered feasible.},
archivePrefix = {arXiv},
arxivId = {1403.5693},
author = {Maclaurin, Dougal and Adams, Ryan P.},
eprint = {1403.5693},
file = {:home/alumbreras/Documentos/Mendeley/Papers/IJCAI International Joint Conference on Artificial Intelligence - 2015 - Maclaurin, Adams - Firefly Monte Carlo Exact MCMC with subsets.pdf:pdf},
isbn = {9781577357384},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {4289--4295},
title = {{Firefly Monte Carlo: Exact MCMC with subsets of data}},
volume = {2015-Janua},
year = {2015}
}
@article{Buntine2006,
abstract = {This article presents a unified theory for analysis of components in discrete data, and compares the methods with techniques such as independent component analysis, non-negative matrix factorisation and latent Dirichlet allocation. The main families of algorithms discussed are a variational approximation, Gibbs sampling, and Rao-Blackwellised Gibbs sampling. Applications are presented for voting records from the United States Senate for 2003, and for the Reuters-21578 newswire collection.},
archivePrefix = {arXiv},
arxivId = {math/0604410},
author = {Buntine, Wray and Jakulin, Aleks},
doi = {10.1007/11752790_1},
eprint = {0604410},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Subspace, Latent Structure and Feature Selection - 2006 - Buntine, Jakulin - Discrete Component Analysis Subspace, Latent Structure and.pdf:pdf},
isbn = {978-3-540-34137-6},
issn = {978-3-540-34137-6},
journal = {Subspace, Latent Structure and Feature Selection},
keywords = {lsa,models,pca,topic},
pages = {1--33},
primaryClass = {math},
title = {{Discrete Component Analysis Subspace, Latent Structure and Feature Selection}},
url = {http://dx.doi.org/10.1007/11752790{\_}1},
volume = {3940},
year = {2006}
}
@article{Mnih2016,
abstract = {Recent progress in deep latent variable models has largely been driven by the development of flexible and scalable variational inference methods. Variational training of this type involves maximizing a lower bound on the log-likelihood, using samples from the variational posterior to compute the required gradients. Recently, Burda et al. (2015) have derived a tighter lower bound using a multi-sample importance sampling estimate of the likelihood and showed that optimizing it yields models that use more of their capacity and achieve higher likelihoods. This development showed the importance of such multi-sample objectives and explained the success of several related approaches. We extend the multi-sample approach to discrete latent variables and analyze the difficulty encountered when estimating the gradients involved. We then develop the first unbiased gradient estimator designed for importance-sampled objectives and evaluate it at training generative and structured output prediction models. The resulting estimator, which is based on low-variance per-sample learning signals, is both simpler and more effective than the NVIL estimator proposed for the single-sample variational objective, and is competitive with the currently used biased estimators.},
archivePrefix = {arXiv},
arxivId = {1602.06725},
author = {Mnih, Andriy and Rezende, Danilo J.},
eprint = {1602.06725},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Arxiv - 2016 - Mnih, Rezende - Variational inference for Monte Carlo objectives.pdf:pdf},
isbn = {1602.06725},
journal = {Arxiv},
pages = {1--3},
title = {{Variational inference for Monte Carlo objectives}},
url = {http://arxiv.org/abs/1602.06725},
volume = {48},
year = {2016}
}
@article{Lefevre2011,
author = {Lef{\`{e}}vre, Augustin and Bach, Francis and F{\'{e}}votte, C{\'{e}}dric},
file = {:home/alumbreras/Documentos/Mendeley/Papers/2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) - 2011 - Lef{\`{e}}vre, Bach, F{\'{e}}votte - Itakura-Saito.pdf:pdf},
isbn = {9781457705397},
journal = {2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
number = {1},
pages = {1--20},
title = {{Itakura-Saito nonnegative matrix factorization with group sparsity}},
year = {2011}
}
@phdthesis{Mohamed2011,
abstract = {Factor analysis and related models for probabilistic matrix factorisation are of central importance to the unsupervised analysis of data, with a colourful history more than a century long. Probabilistic models for matrix factorisation allow us to explore the underlying structure in data, and have relevance in a vast number of application areas including collaborative filtering, source separation, missing data imputation, gene expression analysis, information retrieval, computational finance and computer vision, amongst others. This thesis develops generalisations of matrix factorisation models that advance our understanding and enhance the applicability of this important class of models. The generalisation of models for matrix factorisation focuses on three concerns: widening the applicability of latent variable models to the diverse types of data that are currently available; considering alternative structural forms in the underlying representations that are inferred; and including higher order data structures into the matrix factorisation framework. These three issues reflect the reality of modern data analysis and we develop new models that allow for a principled exploration and use of data in these settings. We place emphasis on Bayesian approaches to learning and the advantages that come with the Bayesian methodology. Our port of departure is a generalisation of latent variable models to members of the exponential family of distributions. This generalisation allows for the analysis of data that may be real-valued, binary, counts, non-negative or a heterogeneous set of these data types. The model unifies various existing models and constructs for unsupervised settings, the complementary framework to the generalised linear models in regression. Moving to structural considerations, we develop Bayesian methods for learning sparse latent representations. We define ideas of weakly and strongly sparse vectors and investigate the classes of prior distributions that give rise to these forms of sparsity, namely the scale-mixture of Gaussians and the spike-and-slab distribution. Based on these sparsity favouring priors, we develop and compare methods for sparse matrix factorisation and present the first comparison of these sparse learning approaches. As a second structural consideration, we develop models with the ability to generate correlated binary vectors. Moment-matching is used to allow binary data with specified correlation to be generated, based on dichotomisation of the Gaussian distribution. We then develop a novel and simple method for binary PCA based on Gaussian dichotomisation. The third generalisation considers the extension of matrix factorisation models to multi-dimensional arrays of data that are increasingly prevalent. We develop the first Bayesian model for non-negative tensor factorisation and explore the relationship between this model and the previously described models for matrix factorisation.},
author = {Mohamed, Shakir},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Unknown - 2011 - Mohamed - Generalised Bayesian matrix factorisation models.pdf:pdf},
keywords = {Bayesian,MCMC,Machine learning,Matrix factorisation,Sparsity,Tensor factorisation},
number = {February},
pages = {1--140},
school = {University of Cambridge},
title = {{Generalised Bayesian matrix factorisation models}},
year = {2011}
}
@article{Thibaux2007,
abstract = {We show that the beta process is the de$\backslash$n$\backslash$nFinetti mixing distribution underlying the In-$\backslash$n$\backslash$ndian buffet process of [2]. This result shows$\backslash$n$\backslash$nthat the beta process plays the role for the$\backslash$n$\backslash$nIndian buffet process that the Dirichlet pro-$\backslash$n$\backslash$ncess plays for the Chinese restaurant process,$\backslash$n$\backslash$na parallel that guides us in deriving analogs$\backslash$n$\backslash$nfor the beta process of the many known ex-$\backslash$n$\backslash$ntensions of the Dirichlet process. In partic-$\backslash$n$\backslash$nular we define Bayesian hierarchies of beta$\backslash$n$\backslash$nprocesses and use the connection to the beta$\backslash$n$\backslash$nprocess to develop posterior inference algo-$\backslash$n$\backslash$nrithms for the Indian buffet process. We also$\backslash$n$\backslash$npresent an application to document classifi-$\backslash$n$\backslash$ncation, exploring a relationship between the$\backslash$n$\backslash$nhierarchical beta process and smoothed naive$\backslash$n$\backslash$nBayes models.},
author = {Thibaux, R and Jordan, M I},
doi = {10.1.1.121.455},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics (AISTATS) - Unknown - Thibaux, Jordan - H.pdf:pdf},
issn = {15324435},
journal = {Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics (AISTATS)},
keywords = {Beta process,Chinese restaurant process,Dirichlet process,Indian buffet process},
title = {{Hierarchical Beta Processes and the Indian Buffet Process}},
year = {2007}
}
@article{Lee1999,
author = {Lee, Daniel D and Seung, H Sebastian},
file = {:home/alumbreras/Documentos/Mendeley/Papers//Nature - 1999 - Lee, Seung - Learning the parts of objects by non-negative matrix factorization.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
keywords = {Algorithms,Face,Humans,Julien,Learning,Models,Neurological,Perception,Perception: physiology,Semantics,TM},
mendeley-tags = {Julien,TM},
month = {oct},
number = {6755},
pages = {788--791},
title = {{Learning the parts of objects by non-negative matrix factorization.}},
volume = {401},
year = {1999}
}
@techreport{Marschner2001,
abstract = {We compare three different stochastic versions of the EM algorithm: The SEM algorithm, the SAEM algorithm and the MCEM algorithm. We suggest that the most relevant contribution of the MCEM methodology is what we call the simulated annealing MCEM algorithm, which turns out to be very close to SAEM. We focus particularly on the mixture of distributions problem. In this context, we review the available theoretical results on the convergence of these algorithms and on the behavior of SEM as the sample size tends to infinity. The second part is devoted to intensive Monte Carlo numerical simulations and a real data study. We show that, for some particular mixture situations, the SEM algorithm is almost always preferable to the EM and simulated annealing versions SAEM and MCEM. For some very intricate mixtures, however, none of these algorithms can be confidently used. Then, SEM can be used as an efficient data exploratory tool for locating significant maxima of the likelihood function. In the real data case, we show that the SEM stationary distribution provides a contrasted view of the loglikelihood by emphasizing sensible maxima.},
author = {Celeux, Gilles and Chauveau, Didier and Diebolt, Jean},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Unknown - 1995 - Celeux, Chauveau, Diebolt - On stochastic versions of the EM algorithm.pdf:pdf},
institution = {INRIA},
issn = {00063444},
title = {{On stochastic versions of the EM algorithm}},
url = {http://www.jstor.org/stable/2673685},
year = {1995}
}
@article{Mnih2007,
author = {Mnih, Andriy and Salakhutdinov, Ruslan},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Advances in neural information processing systems - 2007 - Mnih, Salakhutdinov - Probabilistic matrix factorization.pdf:pdf},
issn = {1049-5258},
journal = {Advances in neural information processing systems},
pages = {1257--1264},
title = {{Probabilistic matrix factorization}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2007{\_}1007.pdf},
year = {2007}
}
@article{Paisley2010a,
abstract = {We show that the stick-breaking construction of the beta process due to Paisley et al. (2010) can be obtained from the characterization of the beta process as a Poisson process. This is achieved by showing that the mean measure of the underlying Poisson process is equal to that of the beta process. We then present a number of consequences of this derivation; in particular, we show how it can be used to derive error bounds on truncated beta processes that are significantly tighter than those in the literature.},
archivePrefix = {arXiv},
arxivId = {1109.0343},
author = {Paisley, John and Zaas, Aimee and Woods, Christopher W and Ginsburg, Geoffrey S and Carin, Lawrence},
eprint = {1109.0343},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Proceedings of the 27th International Conference on Machine Learning (ICML-10) - 2010 - Paisley et al. - A Stick-Breaking Construction o.pdf:pdf},
isbn = {9781605589077},
journal = {Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
pages = {847--854},
title = {{A Stick-Breaking Construction of the Beta Process}},
url = {http://www.icml2010.org/papers/371.pdf},
year = {2010}
}
@article{Fevotte2008,
author = {F{\'{e}}votte, C{\'{e}}dric and Bertin, Nancy and Durrieu, Jean-louis},
doi = {10.1162/neco.2008.04-08-771},
file = {:home/alumbreras/Documentos/Mendeley/Papers/F{\'{e}}votte, Bertin, Durrieu - 2009 - Nonnegative matrix factorization with the Itakura-Saito divergence . With application to music analys.pdf:pdf},
journal = {Neural Computation},
number = {3},
pages = {793--830},
title = {{Nonnegative matrix factorization with the Itakura-Saito divergence . With application to music analysis}},
volume = {21},
year = {2009}
}
@article{Rezende2014a,
abstract = {Abstract We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep , directed generative models , endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition ... $\backslash$n},
archivePrefix = {arXiv},
arxivId = {arXiv:1401.4082v3},
author = {Rezende, D J and Mohamed, S and Wierstra, D},
eprint = {arXiv:1401.4082v3},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Proceedings of The 31st {\ldots} - 2014 - Rezende, Mohamed, Wierstra - Stochastic backpropagation and approximate inference in deep genera(2).pdf:pdf},
isbn = {9781634393973},
journal = {Proceedings of The 31st {\ldots}},
pages = {1278--1286},
title = {{Stochastic backpropagation and approximate inference in deep generative models}},
url = {http://jmlr.org/proceedings/papers/v32/rezende14.html$\backslash$npapers3://publication/uuid/F2747569-7719-4EAC-A5A7-9ECA9D6A8FE6},
volume = {32},
year = {2014}
}
@article{Ranganath2014,
abstract = {Black box inference allows researchers to easily prototype and evaluate an array of models. Re-cent advances in variational inference allow such algorithms to scale to high dimensions. However, a central question remains: How to specify an ex-pressive variational distribution which maintains efficient computation? To address this, we de-velop hierarchical variational models. In a HIERAR-CHICAL VM, the variational approximation is aug-mented with a prior on its parameters, such that the latent variables are conditionally independent given this shared structure. This preserves the computational efficiency of the original approx-imation, while admitting hierarchically complex distributions for both discrete and continuous la-tent variables. We study HIERARCHICAL VM on a variety of deep discrete latent variable models. HI-ERARCHICAL VM generalizes other expressive varia-tional distributions and maintains higher fidelity to the posterior.},
archivePrefix = {arXiv},
arxivId = {1511.02386},
author = {Ranganath, Rajesh and Tran, Dustin and Blei, David M},
eprint = {1511.02386},
file = {:home/alumbreras/Documentos/Mendeley/Papers/arXiv - 2014 - Ranganath, Tran, Blei - Hierarchical Variational Models.pdf:pdf},
isbn = {1511.02386},
journal = {arXiv},
pages = {1--9},
title = {{Hierarchical Variational Models}},
year = {2014}
}
@article{Pereyra2016,
abstract = {Modern signal processing (SP) methods rely very heavily on probability and statistics to solve challenging SP problems. Expectations and demands are constantly rising, and SP methods are now expected to deal with ever more complex models, requiring ever more sophisticated computational inference techniques. This has driven the development of statistical SP methods based on stochastic simulation and optimization. Stochastic simulation and optimization algorithms are computationally intensive tools for performing statistical inference in models that are analytically intractable and beyond the scope of deterministic inference methods. They have been recently successfully applied to many difficult problems involving complex statistical models and sophisticated (often Bayesian) statistical inference techniques. This paper presents a tutorial on stochastic simulation and optimization methods in signal and image processing and points to some interesting research problems. The paper addresses a variety of high-dimensional Markov chain Monte Carlo (MCMC) methods as well as deterministic surrogate methods, such as variational Bayes, the Bethe approach, belief and expectation propagation and approximate message passing algorithms. It also discusses a range of optimization methods that have been adopted to solve stochastic problems, as well as stochastic methods for deterministic optimization. Subsequently, areas of overlap between simulation and optimization, in particular optimization-within-MCMC and MCMC-driven optimization are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1505.00273v1},
author = {Pereyra, Marcelo and Schniter, Philip and Chouzenoux, Emilie and Pesquet, Jean-Christophe and Tourneret, Jean-Yves and Hero, Alfred and McLaughlin, Steve},
doi = {10.1109/JSTSP.2015.2496908},
eprint = {arXiv:1505.00273v1},
file = {:home/alumbreras/Documentos/Mendeley/Papers/IEEE Journal of Selected Topics in Signal Processing - 2016 - Pereyra et al. - A Survey of Stochastic Simulation and Optimization Method.pdf:pdf},
issn = {1932-4553},
journal = {IEEE Journal of Selected Topics in Signal Processing},
number = {2},
pages = {224--241},
title = {{A Survey of Stochastic Simulation and Optimization Methods in Signal Processing}},
url = {http://arxiv.org/abs/1505.00273$\backslash$nhttp://www.arxiv.org/pdf/1505.00273.pdf},
volume = {10},
year = {2016}
}
@article{Lawrence2009,
abstract = {... 601 Page 2. Non - linear Matrix Factorization with Gaussian Processes ... and combining with the likelihood we recover the fol- lowing marginal likelihood: p (Y|X,$\sigma$2,$\alpha$w) = D ∏ j=1 N (y:,j|0,$\alpha$−1 w XX + $\sigma$2I) . ... It is known as dual probabilistic PCA (DPPCA, Lawrence , 2005). ... $\backslash$n},
author = {Lawrence, Neil D and Urtasun, Raquel},
doi = {10.1145/1553374.1553452},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Proceedings of the 26th Annual International Conference on Machine Learning - 2009 - Lawrence, Urtasun - Non-linear matrix factorization.pdf:pdf},
isbn = {9781605585161},
issn = {9781605585161},
journal = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {601--608},
title = {{Non-linear matrix factorization with Gaussian processes}},
url = {http://dl.acm.org/citation.cfm?id=1553374.1553452{\%}5Cnpapers3://publication/doi/10.1145/1553374.1553452},
year = {2009}
}
@article{Celeux1992,
abstract = {The EM algorithm is a widely applicable approach for computing maximum likelihood estimates for incomplete data. We present a stochastic approximation type EM algorithm: SAEM. This algorithm is an adaptation of the stochastic EM algorithm (SEM) that we have previously developed. Like SEM, SAEM overcomes most of the well-known limitations of EM. Moreover, SAEM performs better for small samples. Furthermore, SAEM appears to be more tractable than SEM, since it provides almost sure convergence, while SEM provides convergence in distribution. Here, we restrict attention on the mixture problem. We state a theorem which asserts that each SAEM sequence converges a.s. to a local maximizer of the likelihood function. We close this paper with a comparative study, based on numerical simulations, of these three algorithms.},
author = {Celeux, G and Diebolt, J},
doi = {10.1080/17442509208833797},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Stochastics and Stochastic Reports - 1992 - Celeux, Diebolt - A stochastic approximation type EM algorithm for the mixture problem.pdf:pdf},
issn = {1744-2508},
journal = {Stochastics and Stochastic Reports},
number = {1-2},
pages = {119--134},
title = {{A stochastic approximation type EM algorithm for the mixture problem}},
url = {http://www.tandfonline.com/doi/abs/10.1080/17442509208833797},
volume = {41},
year = {1992}
}
@inproceedings{Canny2004,
address = {Sheffield, U.K.},
author = {Canny, John},
booktitle = {International ACM SIGIR Conference on Research and Development in Information Retrieval},
file = {:home/alumbreras/Documentos/Mendeley/Papers/International ACM SIGIR Conference on Research and Development in Information Retrieval - 2004 - Canny - GaP A Factor Model for Discrete.pdf:pdf},
isbn = {1581138814},
keywords = {em algo-,latent semantic analysis,probabilistic models},
pages = {122--129},
title = {{GaP: A Factor Model for Discrete Data}},
year = {2004}
}
@inproceedings{Dikmen2011,
author = {Dikmen, Onur and F{\'{e}}votte, C{\'{e}}dric},
booktitle = {ICASSP},
file = {:home/alumbreras/Documentos/Mendeley/Papers/ICASSP - 2011 - Dikmen, F{\'{e}}votte - Maximum Marginal Likelihood Estimation for Nonnegative Dictionary Learning.pdf:pdf},
isbn = {9781457705397},
pages = {1992--1995},
title = {{Maximum Marginal Likelihood Estimation for Nonnegative Dictionary Learning}},
year = {2011}
}
@techreport{DuvalMylene2007,
author = {{Duval, Myl{\`{e}}ne} and Robert-Grani{\'{e}}, Christ{\`{e}}le},
file = {:home/alumbreras/Documentos/Mendeley/Papers/Unknown - 2007 - Duval, Myl{\`{e}}ne, Robert-Grani{\'{e}} - SAEM-MCMC some criteria.pdf:pdf},
keywords = {SAEM},
mendeley-tags = {SAEM},
title = {{SAEM-MCMC: some criteria}},
year = {2007}
}
